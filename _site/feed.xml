<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Scott Sherwin Farley</title>
    <description>I am a graduate student at UW Madison studying computing applications to physical geography and paleoecological change.
</description>
    <link>/</link>
    <atom:link href="/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Tue, 18 Apr 2017 10:21:59 -0500</pubDate>
    <lastBuildDate>Tue, 18 Apr 2017 10:21:59 -0500</lastBuildDate>
    <generator>Jekyll v3.1.6</generator>
    
      <item>
        <title>Masters Defense: A general framework for predicting the optimal computing configuration for climate-driven ecological forecasting models</title>
        <description>&lt;p&gt;Yesterday I successfully defended my masters thesis project in both a public talk in front of an audience of about 20 of my friends (and mother!) and in a closed-door session with my committee.&lt;/p&gt;

&lt;p&gt;I think that both portions went quite well. I was smooth and confident in the public talk. I was in with my committee for over one hour – much longer than I was expecting. The conversation during that time was congenial – not the grilling gotcha questions I was expecting. Some of the questions were difficult, but more on a conceptual level about the project’s position in the field. We also talked a lot about the publication venue for the work, and are likely to get at least two pieces of writing out of it.&lt;/p&gt;

&lt;p&gt;I tried to live stream the talk, but unfortunately, the stream came through with no sound. Instead, I’ll include my powerpoint slides here, if you’d like to browse them. Look for a copy of my finished thesis here on my blog later this spring.&lt;/p&gt;

&lt;iframe src=&quot;//www.slideshare.net/slideshow/embed_code/key/wdMmm8GZhg4cUu&quot; width=&quot;595&quot; height=&quot;485&quot; frameborder=&quot;0&quot; marginwidth=&quot;0&quot; marginheight=&quot;0&quot; scrolling=&quot;no&quot; style=&quot;border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;&quot; allowfullscreen=&quot;&quot;&gt; &lt;/iframe&gt;
&lt;div style=&quot;margin-bottom:5px&quot;&gt; &lt;strong&gt; &lt;a href=&quot;//www.slideshare.net/ScottFarley3/a-general-framework-for-predicting-the-optimal-computing-configuration-for-climatedriven-ecological-forecasting-models-scott-farleys-masters-thesis-defense&quot; title=&quot;A general framework for predicting the optimal computing configuration for climate-driven ecological forecasting models: Scott Farley&amp;#x27;s Masters Thesis Defense&quot; target=&quot;_blank&quot;&gt;A general framework for predicting the optimal computing configuration for climate-driven ecological forecasting models: Scott Farley&amp;#x27;s Masters Thesis Defense&lt;/a&gt; &lt;/strong&gt; from &lt;strong&gt;&lt;a target=&quot;_blank&quot; href=&quot;https://www.slideshare.net/ScottFarley3&quot;&gt;Scott Farley&lt;/a&gt;&lt;/strong&gt; &lt;/div&gt;
</description>
        <pubDate>Tue, 18 Apr 2017 10:10:52 -0500</pubDate>
        <link>/research/2017/04/18/Masters-degree-defense.html</link>
        <guid isPermaLink="true">/research/2017/04/18/Masters-degree-defense.html</guid>
        
        
        <category>research</category>
        
      </item>
    
      <item>
        <title>Workshop Presenation: Using Paleobiodiversity Databases for Ancient Land Cover Reconstruction</title>
        <description>&lt;p&gt;Last week I have the opportunity to present a short talk at the Digital Data in Paleontological Research, sponsored by iDigBio in Berkeley, CA. It was a great opportunity to get to know a community of people working with the same types of data as I am, but tackling totally different problems. The goals of my talk were (1) to cast Neotoma and PBDB as excellent additions to modeling pipelines and (2) to introduce my PhD research on reconstructing past land cover. Most of the folks were verebrate paleontologists, and many use image, CT, or 3d databases, rather than the paleobiodiversity databases I am familiar with, so I thought it was important to highlight the ability of these databases to serve a key role on modeling pipelines. Here are the slides from my talk.&lt;/p&gt;

&lt;iframe src=&quot;//www.slideshare.net/slideshow/embed_code/key/aUAu42jQs5D0c0&quot; width=&quot;595&quot; height=&quot;485&quot; frameborder=&quot;0&quot; marginwidth=&quot;0&quot; marginheight=&quot;0&quot; scrolling=&quot;no&quot; style=&quot;border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;&quot; allowfullscreen=&quot;&quot;&gt; &lt;/iframe&gt;
&lt;div style=&quot;margin-bottom:5px&quot;&gt; &lt;strong&gt; &lt;a href=&quot;//www.slideshare.net/ScottFarley3/using-paleobiodiversity-databases-to-reconstruct-past-land-cover&quot; title=&quot;Using paleobiodiversity databases to reconstruct past land cover&quot; target=&quot;_blank&quot;&gt;Using paleobiodiversity databases to reconstruct past land cover&lt;/a&gt; &lt;/strong&gt; from &lt;strong&gt;&lt;a target=&quot;_blank&quot; href=&quot;//www.slideshare.net/ScottFarley3&quot;&gt;Scott Farley&lt;/a&gt;&lt;/strong&gt; &lt;/div&gt;
</description>
        <pubDate>Wed, 29 Mar 2017 11:22:52 -0500</pubDate>
        <link>/research/presentations/2017/03/29/Digital-Data-Paleontology-Presentation.html</link>
        <guid isPermaLink="true">/research/presentations/2017/03/29/Digital-Data-Paleontology-Presentation.html</guid>
        
        
        <category>Research</category>
        
        <category>Presentations</category>
        
      </item>
    
      <item>
        <title>What to do with 360 degrees of longitude?</title>
        <description>&lt;p&gt;I recently spent several days trying to solve a (seemingly) simple problem: vectorizing some rasters. I’m making an interactive of ice sheet volume during deglaciation, and I needed the data in geojson format to put into my mapboxgl map. After finding the &lt;a href=&quot;https://pmip2.lsce.ipsl.fr/design/ice5g/&quot;&gt;data&lt;/a&gt; there were two main problems I encountered:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The data was in NetCDF format.&lt;/li&gt;
  &lt;li&gt;The data had longitude coordinates from 0-360 degrees instead of -180 to 180 degrees.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In my experience, if it’s in NetCDF, you’re going to spend some time getting frustrated trying to work it into a traditional GIS/web mapping workflow. Luckily, gdal has native support for multidimensional netcdfs, so processing isn’t totally impossible.&lt;/p&gt;

&lt;p&gt;The second problem was much more challenging for me and one that required me to download and install no less than four obscure and poorly documented software packages, work in three different languages, and cause general headache. Figure 1 shows a screenshot of the problem.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/bad.png&quot; alt=&quot;Bad&quot; /&gt;
&lt;strong&gt;Figure 1&lt;/strong&gt;: The problem&lt;/p&gt;

&lt;p&gt;In this post, I’ll discuss some of the problems I encountered and how I eventually came upon my solution.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;TL;DR; Use R&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Start Simple&lt;/em&gt; Okay, so I’ve got these weird coordinates in this weird netcdf format. The first logical thing for me to do was to convert to an ASCII &lt;code class=&quot;highlighter-rouge&quot;&gt;.xyz&lt;/code&gt; file so that I could directly manipulate the coordinates. The &lt;code class=&quot;highlighter-rouge&quot;&gt;.xyz&lt;/code&gt; file format is a space-delimited file that lists the x, y, and value (z) data on regular grid. It’s inefficient space-wise, but gives you direct access to both the coordinates and the attributes of a raster file. Using gdal I can convert directly to xyz using something like &lt;code class=&quot;highlighter-rouge&quot;&gt;gdal_translate NETCDF:in.nc:varname -of &quot;XYZ&quot; output.xyz&lt;/code&gt;. Easy. Then I can directly modify the x (longitude) coordinates – coercing them into a -180 to 180 range using the formula:&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;newLongitude&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;oldLongitude&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;180&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;360&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;180&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;This does the correct conversion but isn’t an easily viable technique because of the &lt;code class=&quot;highlighter-rouge&quot;&gt;.xyz&lt;/code&gt; file specification. Specifically,&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;For a same Y coordinate value, the lines in the dataset must be organized by increasing X values. The value of the Y coordinate can increase or decrease however.&lt;/p&gt;

  &lt;p&gt;(GDAL Reference: http://www.gdal.org/frmt_xyz.html)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;So, I’d have to do some nifty sorting to get this to actually work. Moreover, using this method requires direct manipulation of my data’s coordinates, which seems bad.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Combining subsets into single grid&lt;/em&gt;: The next technique I considered was to split the offending portion of the data (longitude &amp;gt; 180) into it’s own tiff file. Make another tiff file for the non-offending longitudes (&amp;lt; 180). Modify the coordinates on the offending file and then merge the two together using &lt;code class=&quot;highlighter-rouge&quot;&gt;gdal_merge.py&lt;/code&gt;. This seemed totally reasonable and very logical. Unfortunately, it caused some weird distortion on the grids, and caused things to not line up properly. I’m not exactly sure what the problem was here, but it caused the dataset to be entirely unusable.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Use Proj4 and &lt;code class=&quot;highlighter-rouge&quot;&gt;gdalwarp&lt;/code&gt;&lt;/em&gt;: Inspired by &lt;a href=&quot;http://gis.stackexchange.com/questions/37790/how-to-reproject-raster-from-0-360-to-180-180-with-cutting-180-meridian&quot;&gt;this post&lt;/a&gt; I tried to use the &lt;code class=&quot;highlighter-rouge&quot;&gt;gdalwarp&lt;/code&gt; to do the coordinate manipulations directly by reprojecting the existing coordinate info. The post suggests:&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;gdalwarp -t_srs WGS84 ~/0_360.tif 180.tif  -wo &lt;span class=&quot;nv&quot;&gt;SOURCE_EXTRA&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;1000 --config CENTER_LONG 0&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;However, if you try do execute this command directly on a netcdf, you get an error: &lt;code class=&quot;highlighter-rouge&quot;&gt;ERROR 1: Input file NETCDF:ice5g_v1.2_03.0k_1deg.nc has no raster bands&lt;/code&gt;. In theory, you could first convert to a 360 degree geotiff, then do this coordinate wrapping, but it seemed like that could cause issues with the coordinates, since geotiffs are not designed to have longitudes &amp;gt; 360. I also tried some stuff with writing new coordinates using proj4 and the lon_wrap parameter.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Do more googling&lt;/em&gt;: It looks like there are packages out there that will do this conversion for you. Most are difficult to figure out exactly how to work them or if they’ll work with netcdfs. I downloaded, installed, and messed with the &lt;a href=&quot;http://www.ncl.ucar.edu/Applications/&quot;&gt;NCAR Command Language (NCL)&lt;/a&gt;, &lt;a href=&quot;http://gmt.soest.hawaii.edu/projects/gmt&quot;&gt;Generic Mapping Tools&lt;/a&gt;, a package of &lt;a href=&quot;https://code.zmaw.de/projects/cdo/embedded/index.html&quot;&gt;Climate Data Operators&lt;/a&gt;, and a promising looking python package. All seem to be (1) poorly documented, (2) hard to use, (3) lacking in examples, (4) extremely esoteric and (5) obscure. &lt;strong&gt;Update&lt;/strong&gt; As I’m writing up this post, it looks like the CDO package actually does work – you can create a new netcdf with rotated grid coordinates using &lt;code class=&quot;highlighter-rouge&quot;&gt;cdo sellonlat,-180,180,-90,90 infile.nc outfile.nc&lt;/code&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Use R&lt;/em&gt;: I should have considered using R earlier in the process, but I was convinced that command line GIS tools would be the way to approach this problem. Nonetheless, R has everything I need: (1) support for netcdf files using the &lt;code class=&quot;highlighter-rouge&quot;&gt;raster&lt;/code&gt; package, (2) geospatial manipulation tools, again in the &lt;code class=&quot;highlighter-rouge&quot;&gt;raster&lt;/code&gt; package (among others), and (3) can write to new formats. In fact, the whole thing can be done in only three lines of R code:&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;raster&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inNC&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;varname&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;varName&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;rotated&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rotate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;writeRaster&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rotated&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;outRaster&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;R definitely appears to be the way to go.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/good.png&quot; alt=&quot;Good&quot; /&gt;
&lt;strong&gt;Figure 2:&lt;/strong&gt; Correct alignment!&lt;/p&gt;
</description>
        <pubDate>Tue, 21 Feb 2017 10:22:52 -0600</pubDate>
        <link>/cartography/analysis/2017/02/21/Netcdf-to-vector.html</link>
        <guid isPermaLink="true">/cartography/analysis/2017/02/21/Netcdf-to-vector.html</guid>
        
        
        <category>cartography</category>
        
        <category>analysis</category>
        
      </item>
    
      <item>
        <title>DC-Mapbox: A Library for Creating Geographically Enabled Dashboards with Mapbox-GL and DC.js</title>
        <description>&lt;p&gt;&lt;img src=&quot;/assets/dc-mapbox-thumb.png&quot; alt=&quot;Screenshot&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I was recently chatting with Shane Loeffler – the brains behind &lt;a href=&quot;http://fc.umn.edu/&quot;&gt;Flyover Country&lt;/a&gt; – who mentioned that they’re upgrading to some new frameworks, including Mapbox-GL. I didn’t want Ice Age Mapper to fall behind, so I figured I should see about integrating vector tiles. Vector tiles are really cool – easy to style, fast, fun to interact with. They also seem to be all the rage in most new mapping applications.&lt;/p&gt;

&lt;p&gt;The new Ice Age Mapper (v2.0 onwards) has hard dependencies on lots of things, including Crossfilter and DC.js. DC.js and Leaflet play nicely together via the &lt;a href=&quot;https://github.com/dc-js/dc.leaflet.js&quot;&gt;DC.leaflet&lt;/a&gt; plugin, which enables dashboard with geographic information to have either a points layer or choropleth layer on a Leaflet map attached to the rest of the chart group. Unfortunately, the new Mapbox-GL library, which enables GPU-leveraged vector tiles, didn’t have nicely developed bindings to DC.js. I think I could have used the &lt;a href=&quot;https://github.com/mapbox/mapbox-gl-leaflet&quot;&gt;Mapbox-GL-Leaflet&lt;/a&gt; bindings and then put the DC-Leaflet map on top, but this would have come at the price of some of the cool features of using Mapbox-GL in the first place.&lt;/p&gt;

&lt;p&gt;Since I wanted to maintain the look and feel of the existing version of Ice Age Mapper, while supporting things like pitch and rotation and fractional zoom, I decided the best course of action was to write the bindings myself. What resulted was this plugin here. While it’s far from perfect, it does the job of linking together a DC.js dashboard and a Mapbox-GL map.&lt;/p&gt;

&lt;p&gt;When the map is moved, an event is triggered to redraw all the other charts in the dashboard with only the points currently in the map view. Similarly, when another chart is brushed, the map is filtered and redrawn to include only points meeting the filter criteria. Filtering is all done via crossfilter, which is super efficient at handling large multidimensional datasets. Filter events are handled via DC, which enabled all of the charts at once to be updated and redrawn.&lt;/p&gt;

&lt;p&gt;There were a couple tricky parts to this project. The first was to filter the map using the &lt;code class=&quot;highlighter-rouge&quot;&gt;layer&lt;/code&gt; on the map. Since, in Mapbox’s new gl tools, the data is not an overlay, but part of the map data, it’s not as simple as iterating through existing markers to see if they should be included in the current view. Instead, I had to first convert everything to geojson, add it as a map source, and render it as a map layer. Then, when a filter event was received, I used the &lt;code class=&quot;highlighter-rouge&quot;&gt;map.setFilter&lt;/code&gt; method on the geojson layer to filter appropriately. Also tricky was adding popups. Since crossfilter reduces dimensions to groups, you loose the contextual information you might want to display in a popup. While it’s kind of hacky – and can cause performance issues – I used the crossfilter dimension instead of the crossfilter group to plot the points on the map, which lets use have popups with contextual information. Finally, the map events are now asynchronous, which means that settings are harder to apply using the getter/setter methods common in D3/DC, since they have to wait until after the map and layer are loaded to be applied. To overcome this, I used a &lt;code class=&quot;highlighter-rouge&quot;&gt;mapOptions&lt;/code&gt; argument in the chart constructor, so properties can be set directly on map initialization.&lt;/p&gt;

&lt;p&gt;There’s a live demo &lt;a href=&quot;http://scottsfarley.com/dc-mapbox/examples&quot;&gt;here&lt;/a&gt; and some annotated source and example code on my &lt;a href=&quot;http://github.com/scottsfarley93/dc-mapbox&quot;&gt;github&lt;/a&gt;. Tell me what you think and feel free to contribute if you like what you see or think it could be better.&lt;/p&gt;
</description>
        <pubDate>Sat, 11 Feb 2017 11:00:00 -0600</pubDate>
        <link>/cartography/coding/2017/02/11/DC-Mapbox-Plugin.html</link>
        <guid isPermaLink="true">/cartography/coding/2017/02/11/DC-Mapbox-Plugin.html</guid>
        
        
        <category>cartography</category>
        
        <category>coding</category>
        
      </item>
    
      <item>
        <title>Proportional Clusters: A Plugin for Leaflet</title>
        <description>&lt;p&gt;&lt;img src=&quot;/assets/pclusters.png&quot; alt=&quot;Screenshot&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I’ve been working hard on Ice Age Mapper lately, and I’ve been thinking a lot about the symbolization of the points on the map. I’ve done a lot for the representation of the other dimensions, by using the multidimensional dashboards in the new analytics panel. But, the map just has so many points on it (for a lot of taxa) that it’s hard to figure out how and if to group them. One of the things I came up with is to make a multi-scale proportional symbol map that clusters the points and makes a symbol sized according to how many children are in the cluster. Drawing heavily on the &lt;a href=&quot;https://github.com/Leaflet/Leaflet.markercluster&quot;&gt;Leaflet.MarkerCluster&lt;/a&gt; plugin, I developed a separate plugin that allows you to make a proportional cluster map with your own data. It includes options for scaling the radius of the symbols and styling the points.&lt;/p&gt;

&lt;p&gt;You can see a live demo &lt;a href=&quot;http://scottsfarley.com/leaflet.proportionalClusters&quot;&gt;here&lt;/a&gt; and you can find the annotated source for the library and example &lt;a href=&quot;http://github.com/scottsfarley93/leaflet.proportionalClusters&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;There’s a few more things I want to add to the library. Right now, it’s got a hard dependency on d3, to do the radius scaling. If I can port the d3 scale module into this library, it would remove that, making it a better standalone module. I’d also like to improve the styling, which is currently a combination of jQuery and CSS. Ideally, I’d like to remove all dependence on external stylesheets, and have everything be styled by calls to the constructor function.&lt;/p&gt;
</description>
        <pubDate>Sat, 11 Feb 2017 10:22:52 -0600</pubDate>
        <link>/cartography/coding/2017/02/11/Prop-clusters-library.html</link>
        <guid isPermaLink="true">/cartography/coding/2017/02/11/Prop-clusters-library.html</guid>
        
        
        <category>cartography</category>
        
        <category>coding</category>
        
      </item>
    
      <item>
        <title>Converting NetCDF to Raster</title>
        <description>&lt;p&gt;Sometimes, especially when working with climate data, it is necessary to work with NetCDFs (Network Common Data Format – standard data packaging for scientific data).  While NetCDF is good for data distribution, storage, and provenance, it is less good for working into standard raster data processing workflows. If you’re used to working with Tiffs or JPEGs, the following command might be helpful, assuming you have gdal installed on your machine.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;c&quot;&gt;# convert the specified dimension to Raster&lt;/span&gt;
gdal_translate NETCDF:[PATH_TO_NC_FILE]:[DIMENSION_OF_INTEREST] -of &lt;span class=&quot;s2&quot;&gt;&quot;[FORMAT_DESIRED]&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;OUTPUT_FILE_NAME]&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;This will create an N-banded raster with one band for each time dimension related in your dataset.&lt;/p&gt;

&lt;p&gt;For example, to convert a netcdf file called &lt;code class=&quot;highlighter-rouge&quot;&gt;trace.01-36.22000BP.csim.aice.22000BP_decavg_400BCE.nc&lt;/code&gt; (climate data from the SynTrace paleoclimate experiment) using the dimension &lt;code class=&quot;highlighter-rouge&quot;&gt;aice&lt;/code&gt; (area of gridcell covered in glacier) to a tiff (&lt;code class=&quot;highlighter-rouge&quot;&gt;GTiff&lt;/code&gt;), I might enter:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;gdal_translate NETCDF:trace.01-36.22000BP.csim.aice.22000BP_decavg_400BCE.nc:aice -of &lt;span class=&quot;s2&quot;&gt;&quot;GTiff&quot;&lt;/span&gt; aice.tif&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;This creates a 2204 band raster representing ice area in the tiff format. Super handy for then converting into a format to be digested by a web application.&lt;/p&gt;
</description>
        <pubDate>Sun, 05 Feb 2017 11:00:00 -0600</pubDate>
        <link>/research/data/cartography/2017/02/05/Converting-Netcdf-To-Raster.html</link>
        <guid isPermaLink="true">/research/data/cartography/2017/02/05/Converting-Netcdf-To-Raster.html</guid>
        
        
        <category>Research</category>
        
        <category>Data</category>
        
        <category>Cartography</category>
        
      </item>
    
      <item>
        <title>A Whirlwind Tour to Creating Your Own Data Service Using Postgres and Javascript</title>
        <description>&lt;p&gt;I led this week’s edition of the &lt;a href=&quot;http://www.geography.wisc.edu/cartography/&quot;&gt;Cart Lab Education Series&lt;/a&gt; with a quick tutorial on how to go about creating an application backend (or Data Service or API, depending on your terminology) using JavaScript (Node.js) and Postgres. It was a lot to cover in less than half an hour, so I wanted to write up a quick blog post about it too. To lead the discussion, I created this infographic that outlines the components of a data service, and how they might relate to one another.&lt;/p&gt;

&lt;embed height=&quot;800&quot; width=&quot;100%&quot; src=&quot;/assets/cles_Jan_30.pdf&quot; /&gt;

&lt;p&gt;As a hands on demo, I created a database with some example data from the &lt;a href=&quot;http://www.transtats.bts.gov/DL_SelectFields.asp?Table_ID=236&amp;amp;DB_Short_Name=On-Time&quot;&gt;Flight Delay Dataset&lt;/a&gt;, and wrote up a couple annotated code examples.  The code examples are on &lt;a href=&quot;https://github.com/scottsfarley93&quot;&gt;my GitHub&lt;/a&gt;, as well as a SQL dump of the database, if you want to follow along at home.  If you’re at the University of Wisconsin, you can query the database I am hosting on the Geography Department’s server.&lt;/p&gt;

&lt;h3 id=&quot;setting-up-the-database&quot;&gt;Setting up the Database&lt;/h3&gt;

&lt;p&gt;To have a data service, you need a data base.  The database can use any RMDBS you want (MySQL, PostGres, SQLite, MongoDB).  You should think about the system you want to use and the schema you want to implement before you start.  That’s a theme for another CLES, or Qunying’s Geography 576.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Install the Database Management System&lt;/li&gt;
  &lt;li&gt;Create user roles and privileges&lt;/li&gt;
  &lt;li&gt;Think about a relational table structure that makes sense for your data&lt;/li&gt;
  &lt;li&gt;Implement the schema&lt;/li&gt;
  &lt;li&gt;Add data (may require massaging your data, and often requires a script)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;setting-up-the-bridge&quot;&gt;Setting up the Bridge&lt;/h3&gt;
&lt;p&gt;The second component of the data service is the ‘bridge’, or what other people might call an API or application backend. I like the term bridge, because it makes clear its role as linking the database and the application client.  Call it what you like. I like to use Node.js for my bridge programs, here are some reasons: it’s fast, it’s popular, and it’s JavaScript. I find it a lot easier to write the full stack in a single language than going back and forth between different scripting languages. That being said, there are lots of other choices for language: Python, ASP, .NET, PHP.  I’ve written similar programs in both python and PHP and it’s not that different. Choose what you’re comfortable with.&lt;/p&gt;

&lt;p&gt;If you do go with node, here are the major steps (detailed below) that lead to a finished data service:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Install Node (https://nodejs.org/en/)&lt;/li&gt;
  &lt;li&gt;Create a new application using &lt;code class=&quot;highlighter-rouge&quot;&gt;npm init&lt;/code&gt; (or clone my repo)&lt;/li&gt;
  &lt;li&gt;Set up the application in &lt;code class=&quot;highlighter-rouge&quot;&gt;app.js&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Write controllers for each endpoint&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;creating-a-new-node-application&quot;&gt;Creating a new Node Application&lt;/h3&gt;
&lt;p&gt;All Node programs are also node packages, which are managed by the node package manager (&lt;code class=&quot;highlighter-rouge&quot;&gt;npm&lt;/code&gt;).  There’s a handy wizard that will guide you through the creation of a new package in &lt;code class=&quot;highlighter-rouge&quot;&gt;npm init&lt;/code&gt;.  Our program (and yours probably, if it does anything interesting) uses other people’s libraries.  There are node libraries for everything: compression, password hashing, file system access, you name it.  When in doubt, use a module, don’t try to write it yourself.  Once we’ve created the new package, we tell it about the libraries we require (these are called dependencies), by adding a section in &lt;code class=&quot;highlighter-rouge&quot;&gt;package.json&lt;/code&gt;.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Initialize new project with &lt;code class=&quot;highlighter-rouge&quot;&gt;npm init&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Follow prompts on terminal screen to initialize new application&lt;/li&gt;
  &lt;li&gt;Open &lt;code class=&quot;highlighter-rouge&quot;&gt;package.json&lt;/code&gt; with your favorite text editor.&lt;/li&gt;
  &lt;li&gt;Add the following to include dependencies:&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;&lt;span class=&quot;w&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;dependencies&quot;&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;express&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;latest&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;body-parser&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;latest&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;pg&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;latest&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;pg-promise&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;latest&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;This installs the following libraries:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;express&lt;/code&gt;: Web framework&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;body-parser&lt;/code&gt;: For getting query data&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;pg&lt;/code&gt;: Postgres bindings&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;pg-promise&lt;/code&gt;: Postgres bindings with better syntax&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Run the command &lt;code class=&quot;highlighter-rouge&quot;&gt;npm install&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;You’re now able to run your application. If you (at any time) need to add more dependencies, add them to the section you just created.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;set-up-the-application&quot;&gt;Set up the application&lt;/h3&gt;
&lt;p&gt;The bridge application will do the actual work on parsing user input values, writing SQL, executing queries, receiving database output, and packaging it up for the client.  Everything will be done in JavaScript functions, and will live in &lt;code class=&quot;highlighter-rouge&quot;&gt;app.js&lt;/code&gt;. (If you write a complex service, or use a tool like &lt;a href=&quot;http://swagger.io/&quot;&gt;Swagger&lt;/a&gt;, you may put pieces of your code into different files.  Not now.) Key parts of this file include importing libraries, connecting to the database using your credentials, and writing function for each service endpoint (next section).&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Create a file called &lt;code class=&quot;highlighter-rouge&quot;&gt;app.js&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Import the required modules (this part is a lot like python) by creating a new variable with its name:&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-javascript&quot; data-lang=&quot;javascript&quot;&gt;    &lt;span class=&quot;kd&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;express&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;require&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&#39;express&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;Create a connection to the database.
    &lt;ul&gt;
      &lt;li&gt;For this, I like to use a function, so it can be reused. For example:&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-javascript&quot; data-lang=&quot;javascript&quot;&gt;    &lt;span class=&quot;nx&quot;&gt;createDBConnection&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(){&lt;/span&gt;
      &lt;span class=&quot;c1&quot;&gt;//returns a database connection&lt;/span&gt;
        &lt;span class=&quot;kd&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;cn&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;host&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;yourHost&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;port&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5432&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;//default&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;database&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;yourDBName&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;user&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;yourUsername&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;password&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;yourPassword&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;};&lt;/span&gt;
    &lt;span class=&quot;kd&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;db&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;pgp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;cn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;//do the connection using pg-promise library&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;db&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;Write endpoint functions.&lt;/li&gt;
  &lt;li&gt;Start your application. Tell the server to listen for incoming client requests. I like to develop on ports 8000 and 8080.  Apache (web server) uses 80 (default in web browsers).&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-javascript&quot; data-lang=&quot;javascript&quot;&gt;    &lt;span class=&quot;nx&quot;&gt;app&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;listen&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8080&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;function&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
      &lt;span class=&quot;nx&quot;&gt;console&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&#39;Started application.&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;
    &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h3 id=&quot;writing-endpoint-functions&quot;&gt;Writing endpoint functions&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;In the body of your &lt;code class=&quot;highlighter-rouge&quot;&gt;app.js&lt;/code&gt; file, write a function for each endpoint you want to support. For now, we’ll just focus on &lt;code class=&quot;highlighter-rouge&quot;&gt;GET&lt;/code&gt; requests, but you can do other verbs as well (most importantly, you can add data to the database with &lt;code class=&quot;highlighter-rouge&quot;&gt;POST&lt;/code&gt; requests). Also, for now we’ll look at getting data from the query string, but you can also get data passed in via the body of the request if you want that too…&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If we want to give a user a lost of the flights (day, time, departure city, arrival city, delay amount, we might want to have an endpoint called &lt;code class=&quot;highlighter-rouge&quot;&gt;flights&lt;/code&gt; that a user would access by going to &lt;code class=&quot;highlighter-rouge&quot;&gt;https://my.website.com/api/flights&lt;/code&gt;.  That way the user knows for sure she’s getting the flights.  It’s a good idea to organize your endpoints into logical groups of what they return.&lt;/p&gt;

&lt;p&gt;However, our user might not want all the flights, rather she wants to query for those from or to a particular city, or to limit the number of results coming back from the database.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Start of by writing a function like this, for a &lt;code class=&quot;highlighter-rouge&quot;&gt;GET&lt;/code&gt; request to the &lt;code class=&quot;highlighter-rouge&quot;&gt;/flights&lt;/code&gt; service.&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-javascript&quot; data-lang=&quot;javascript&quot;&gt;  &lt;span class=&quot;nx&quot;&gt;app&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&#39;/flights&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;function&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;req&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;res&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;//this function is the endpoint for the flight data&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;//do step two here&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;//do step three here&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;//do step four here&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;//do step five here&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;

  &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;Inside of the function body, parse the user input – how are they searching or filtering:&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-javascript&quot; data-lang=&quot;javascript&quot;&gt;  &lt;span class=&quot;c1&quot;&gt;//get query parameters&lt;/span&gt;
  &lt;span class=&quot;kd&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;originCity&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;req&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;query&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;originCity&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;||&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;null&lt;/span&gt;
  &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Include one variable for each filter/query parameter/argument you want in your endpoint. This gives you the value of that parameter, if it’s in the query string, otherwise, you get a &lt;code class=&quot;highlighter-rouge&quot;&gt;null&lt;/code&gt;.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Write a SQL query, using the value other the parameter(s) given by the user. The SQL here is arbitrary – you can do anything you might want to do in PGAdmin or psql.  Joins, views, selects, deletes – it’s all on the table. Pass user parameter values in via the &lt;code class=&quot;highlighter-rouge&quot;&gt;${variableName}&lt;/code&gt; syntax, or see the &lt;a href=&quot;https://github.com/vitaly-t/pg-promise&quot;&gt;pg-promise&lt;/a&gt; library docs.&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sql&quot; data-lang=&quot;sql&quot;&gt;  &lt;span class=&quot;k&quot;&gt;sql&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;&quot;SELECT * FROM flightdelays &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;\&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;
    WHERE 1=1
    AND (${origin} IS NULL or flightdelays.origin = ${origin})&quot;&lt;/span&gt;
  &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;This syntax ensures that all results will be given back to the user in the case that no origin city is specified, and is extremely helpful for API building.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Execute the query, using the pg-promise functions. This happens asynchronously, so be prepared. You can make an object with the query values first, if that helps you think through what you’re passing to the query.&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-javascript&quot; data-lang=&quot;javascript&quot;&gt;  &lt;span class=&quot;nx&quot;&gt;db&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;any&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;sql&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;origin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;originCity&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;then&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kd&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
      &lt;span class=&quot;c1&quot;&gt;//happens on success&lt;/span&gt;
      &lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;catch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kd&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;err&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
      &lt;span class=&quot;c1&quot;&gt;//happens on error&lt;/span&gt;
      &lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;
  &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;Do any data conversions or other stuff you want. Then return the result as JSON. I like to include a timestamp, and a message that says the call succeeded.&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-javascript&quot; data-lang=&quot;javascript&quot;&gt;  &lt;span class=&quot;c1&quot;&gt;//return response to user&lt;/span&gt;
  &lt;span class=&quot;kd&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;ts&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;Date&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;toJSON&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
  &lt;span class=&quot;kd&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;resOut&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;s2&quot;&gt;&quot;success&quot;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;s2&quot;&gt;&quot;timestamp&quot;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;ts&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;data&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
  &lt;span class=&quot;nx&quot;&gt;res&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;json&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;resOut&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;//finish request by sending data back to the user&lt;/span&gt;

  &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h3 id=&quot;start-the-service&quot;&gt;Start the Service&lt;/h3&gt;
&lt;p&gt;Now that you’ve built your awesome data service, you’ll need to start it. Here’s how:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Open a new terminal/command line window&lt;/li&gt;
  &lt;li&gt;Run the command &lt;code class=&quot;highlighter-rouge&quot;&gt;node app.js&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;The server is running.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;setting-up-the-client&quot;&gt;Setting up the Client&lt;/h2&gt;
&lt;p&gt;The client can be set up as you would for any AJAX call. No special modifications are needed, you just need to know the names and data types of the values being passed in via the query string.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-javascript&quot; data-lang=&quot;javascript&quot;&gt;&lt;span class=&quot;nx&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;ajax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;localhost:8080&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;origin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&#39;PHX&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;success&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
      &lt;span class=&quot;nx&quot;&gt;console&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Got data!&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;nx&quot;&gt;console&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;
    &lt;span class=&quot;nx&quot;&gt;error&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;status&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;xhr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;error&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
      &lt;span class=&quot;nx&quot;&gt;console&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;xhr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;responseText&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;
  &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2 id=&quot;next-steps&quot;&gt;Next Steps:&lt;/h2&gt;
&lt;p&gt;If you’ve made it this far, you might want to try some more challenging tasks, such as:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Check out the annotated source in &lt;code class=&quot;highlighter-rouge&quot;&gt;app.js&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;Add a new query parameter to filter destination city.&lt;/li&gt;
  &lt;li&gt;Add a new query parameter to include only those results that have more than a certain delay (in minutes).&lt;/li&gt;
  &lt;li&gt;Add a new endpoint that lists the airports in the dataset and their states and cities.&lt;/li&gt;
  &lt;li&gt;Add a new endpoint that summarizes the delays by airport.&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Tue, 31 Jan 2017 11:00:00 -0600</pubDate>
        <link>/research/education/teaching/2017/01/31/Creating-a-data-service.html</link>
        <guid isPermaLink="true">/research/education/teaching/2017/01/31/Creating-a-data-service.html</guid>
        
        
        <category>Research</category>
        
        <category>Education</category>
        
        <category>Teaching</category>
        
      </item>
    
      <item>
        <title>Climate Data Service Released (alpha)</title>
        <description>&lt;p&gt;Several months back, I wrote about the &lt;a href=&quot;http://scottsfarley.com/research/paleoclimate/2016/06/26/the-niche-api-web-service.html&quot;&gt;niche API&lt;/a&gt; that I was putting together. I put that on hold for a hot minute while I was working on other projects, most notably, my thesis. Since the &lt;a href=&quot;http://fc.umn.edu&quot;&gt;Flyover Country&lt;/a&gt; team came to Madison in November, I’ve been pretty psyched up about it again, and have been working on it consistently for several weeks.  I’m pretty confident in it’s current version, and I hope you head over to the &lt;a href=&quot;http://grad.wisc.edu/cds&quot;&gt;live demo&lt;/a&gt; page or the &lt;a href=&quot;http://github.com/scottsfarley93/niche-api&quot;&gt;github&lt;/a&gt; to check it out.&lt;/p&gt;

&lt;p&gt;I attended the AGU annual fall meeting this week. I browsed the informatics talks and posters every day, which convinced me a data service like this is something that nobody is doing, and something that could be really special. Most folks are focused on NetCDF distribution and metadata. NetCDFs are great, and are (and will remain) and important method of disseminating climate model and earth observation data.  However, it doesn’t allow programmatic access to arbitrary geometries. OpenDAP and other protocols let you extract some subsets of a remote NetCDF, but you’ve still got to deal with a grid object on the client end.  Putting data in a more structured database (or even unstructured – there’s potential for a graph/nosql database implementation here) allows clients to request portions of single or multiple bands along sets of points, lines, or polygons.  It opens the potential for statistics (mean, median, variance, etc) within a geometry for a climate variable, allowing you to aggregate over space or time. Finally, it makes life way easier on the client side: rather than having to manipulate a grid (whole or subset), the client can consume exactly what they want.  For example, plotting time series, histograms, or environmental space plots in-browser with javascript.&lt;/p&gt;

&lt;p&gt;I think that (with a lot more work) this type of service could fundamentally change the way that people interact with climate model data.&lt;/p&gt;

&lt;p&gt;I think that important things to take care of now are standardizing the variables and source metadata to make my system consistent with leading metadata standards.  Particularly, I think I’ll work to conform my variable naming conventions to the NetCDF-CF metadata standard for variable names, and use the NASA Global Change Master Directory standards for temporal and spatial resolution conventions. I’m still looking around for climate model metadata standards, but I want to be able to differentiate the forcing scenarios, model versions, and resolutions.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://goring.org&quot;&gt;Simon Goring&lt;/a&gt; and I spent some time talking about interesting projects that could demonstrate the utility of this project. I’m not used to thinking of things in terms of papers that could be written, but it seems like there’s at least one that could be viable. I tend to like to think of things in terms of their utility to the general community – not exactly the way of the world in academia. Some potential things that we came up with:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Correlate climate reconstructions build using transfer functions from data in the Neotoma Database with climate model output. This would simplify the process of both building the transfer function (you could hit the data service for modern climate) as well as for comparison against models (you could hit the exact space-time locations).  We could then build a map of correlation between GCM-modeled climates and pollen-reconstructed climates.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Understand point-specific uncertainty in climate model projections to the end of the century. We know there is significant uncertainty both in the scenario (how much CO_2 in the atmosphere) and in the individual dynamics of the models. The climate data service could help to develop an understanding of inter-model variability at specific points.  For example, given business as usual (e.g., RCP8.5), what are the range of possible climates that Paris might experience in 2050, 2075, and 2100. Again, such an analysis would be possible without the development of my API, but it would enable public, programmatic consumption and, perhaps, promote broader public understanding.  For example, what if you could serve a visualization web-app where uncertainty ranges for any user-entered point? That’d be cool.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;There’s so much more I’d like to do on this.  Particularly, on the top of my list are:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Redesign temporal data model. Not all data will have the same resolution, and not all will have 1950 as an appropriate benchmark time. Develop a new temporal data model so that the system can handle both past and future data, as well as data that only represents a single point in time.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Ingest more models. Currently, I’ve only got the Lorenz et al. downscaled CCSM 3 data from North America. I’d like to ingest at least one more data source in the near future as a proof of concept for model inter-comparison.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Source, model, and variable metadata development. Enhance metadata so that variables and sources are well defined and can be programmatically consumed.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Develop an automated ingestion pipeline.  Allow authenticated users (me) to upload raw data (NetCDFs, raster grids, etc), convert them to the proper format, then ingest the data and the metadata into the database.  This could vastly improve the speed at which I added to the database, since everything is done by interactively now. It requires a lot of complex geoprocessing on the server though – I don’t know if the geography department will be okay with that…&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;If this sounds interesting to you, let me know! If you want to consume my data service, that’d be awesome. I’d be happy to modify the existing returns to fit your needs. Here are up-to-date &lt;a href=&quot;http://grad.geography.wisc.edu/cds/docs&quot;&gt;docs&lt;/a&gt; with examples. If you want to help develop, that’d be awesome too. I’m super excited about this project, but pushing the limits of what I know. Send me an &lt;a href=&quot;&amp;#109;&amp;#097;&amp;#105;&amp;#108;&amp;#116;&amp;#111;:&amp;#115;&amp;#102;&amp;#097;&amp;#114;&amp;#108;&amp;#101;&amp;#121;&amp;#050;&amp;#064;&amp;#119;&amp;#105;&amp;#115;&amp;#099;&amp;#046;&amp;#101;&amp;#100;&amp;#117;&quot;&gt;email&lt;/a&gt; – let’s talk.&lt;/p&gt;
</description>
        <pubDate>Fri, 16 Dec 2016 07:22:52 -0600</pubDate>
        <link>/research/data/2016/12/16/Climate-Data-Service.html</link>
        <guid isPermaLink="true">/research/data/2016/12/16/Climate-Data-Service.html</guid>
        
        
        <category>Research</category>
        
        <category>Data</category>
        
      </item>
    
      <item>
        <title>2016 NACIS Dynamic Mapping Award Win!</title>
        <description>&lt;p&gt;I’m psyched to put out that a map that I worked on last spring won the North American Cartographic Society Best Dynamic Map award at their annual conference a couple weeks back.  It was a blast working with Starr and Meghan on this project and well worth the late nights. Looking forward to more interactive mapping coming up.  If you’ve got something you want to map – let me know!&lt;/p&gt;

&lt;p&gt;Read more about the application &lt;a href=&quot;http://nacis.org/awards/2016-winner-wooden-ships/&quot;&gt;here&lt;/a&gt; or explore it here.&lt;/p&gt;

&lt;iframe src=&quot;http://wooden-ships.github.io/&quot; style=&quot;height: 1000px; width:100%&quot; /&gt;

</description>
        <pubDate>Thu, 03 Nov 2016 09:22:52 -0500</pubDate>
        <link>/tutorial/2016/11/03/Nacis-Award.html</link>
        <guid isPermaLink="true">/tutorial/2016/11/03/Nacis-Award.html</guid>
        
        
        <category>Tutorial</category>
        
      </item>
    
      <item>
        <title>Occurrences in Neotoma: 18,900,000 and counting...</title>
        <description>&lt;p&gt;A few weeks ago, I posted about Big Data in the field of ecology, specifically biodiversity informatics, where I looked at the holdings of the Neotoma Paleoecological Database and the Global Biodiversity Information Facility (GBIF). I made some comparisons between the two databases, though the units of scale were different. In Neotoma, I used the number of &lt;em&gt;datasets&lt;/em&gt; that had been submitted, while for GBIF, I commented on the number of &lt;em&gt;occurrences&lt;/em&gt;.  These are two fundamentally different units, so I set out to resolve the issue and find out how many occurrences there are in Neotoma.&lt;/p&gt;

&lt;p&gt;Counting every single taxa in every single level in Neotoma, there are over 18.9 million occurrence records – identifications of a single type at a single space-time locus.&lt;/p&gt;

&lt;p&gt;To be exact, there are 18,903,236 occurrence records, with a steeply increasing trend over the last several months. I suspect we’ll be up over 20 million by the middle of 2017.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/bigData/Neotoma_Occurrences.png&quot; alt=&quot;NeotomaOccurrenceRecords&quot; /&gt;&lt;/p&gt;
</description>
        <pubDate>Sat, 22 Oct 2016 09:22:52 -0500</pubDate>
        <link>/tutorial/2016/10/22/18-Million-Neotoma-Occurrences.html</link>
        <guid isPermaLink="true">/tutorial/2016/10/22/18-Million-Neotoma-Occurrences.html</guid>
        
        
        <category>Tutorial</category>
        
      </item>
    
  </channel>
</rss>
