<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Scott&#39;s Blog</title>
    <description>I am a graduate student at UW Madison studying computing applications to physical geography and paleoecological change.
</description>
    <link>/</link>
    <atom:link href="/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Mon, 06 Feb 2017 19:20:28 -0600</pubDate>
    <lastBuildDate>Mon, 06 Feb 2017 19:20:28 -0600</lastBuildDate>
    <generator>Jekyll v3.1.6</generator>
    
      <item>
        <title>Converting NetCDF to Raster</title>
        <description>&lt;p&gt;Sometimes, especially when working with climate data, it is necessary to work with NetCDFs (Network Common Data Format – standard data packaging for scientific data).  While NetCDF is good for data distribution, storage, and provenance, it is less good for working into standard raster data processing workflows. If you’re used to working with Tiffs or JPEGs, the following command might be helpful, assuming you have gdal installed on your machine.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;c&quot;&gt;# convert the specified dimension to Raster&lt;/span&gt;
gdal_translate NETCDF:[PATH_TO_NC_FILE]:[DIMENSION_OF_INTEREST] -of &lt;span class=&quot;s2&quot;&gt;&quot;[FORMAT_DESIRED]&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;OUTPUT_FILE_NAME]&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;This will create an N-banded raster with one band for each time dimension related in your dataset.&lt;/p&gt;

&lt;p&gt;For example, to convert a netcdf file called &lt;code class=&quot;highlighter-rouge&quot;&gt;trace.01-36.22000BP.csim.aice.22000BP_decavg_400BCE.nc&lt;/code&gt; (climate data from the SynTrace paleoclimate experiment) using the dimension &lt;code class=&quot;highlighter-rouge&quot;&gt;aice&lt;/code&gt; (area of gridcell covered in glacier) to a tiff (&lt;code class=&quot;highlighter-rouge&quot;&gt;GTiff&lt;/code&gt;), I might enter:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;gdal_translate NETCDF:trace.01-36.22000BP.csim.aice.22000BP_decavg_400BCE.nc:aice -of &lt;span class=&quot;s2&quot;&gt;&quot;GTiff&quot;&lt;/span&gt; aice.tif&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;This creates a 2204 band raster representing ice area in the tiff format. Super handy for then converting into a format to be digested by a web application.&lt;/p&gt;
</description>
        <pubDate>Sun, 05 Feb 2017 11:00:00 -0600</pubDate>
        <link>/research/data/cartography/2017/02/05/Converting-Netcdf-To-Raster.html</link>
        <guid isPermaLink="true">/research/data/cartography/2017/02/05/Converting-Netcdf-To-Raster.html</guid>
        
        
        <category>Research</category>
        
        <category>Data</category>
        
        <category>Cartography</category>
        
      </item>
    
      <item>
        <title>A Whirlwind Tour to Creating Your Own Data Service Using Postgres and Javascript</title>
        <description>&lt;p&gt;I led this week’s edition of the &lt;a href=&quot;http://www.geography.wisc.edu/cartography/&quot;&gt;Cart Lab Education Series&lt;/a&gt; with a quick tutorial on how to go about creating an application backend (or Data Service or API, depending on your terminology) using JavaScript (Node.js) and Postgres. It was a lot to cover in less than half an hour, so I wanted to write up a quick blog post about it too. To lead the discussion, I created this infographic that outlines the components of a data service, and how they might relate to one another.&lt;/p&gt;

&lt;p&gt;As a hands on demo, I created a database with some example data from the &lt;a href=&quot;http://www.transtats.bts.gov/DL_SelectFields.asp?Table_ID=236&amp;amp;DB_Short_Name=On-Time&quot;&gt;Flight Delay Dataset&lt;/a&gt;, and wrote up a couple annotated code examples.  The code examples are on &lt;a href=&quot;https://github.com/scottsfarley93&quot;&gt;my GitHub&lt;/a&gt;, as well as a SQL dump of the database, if you want to follow along at home.  If you’re at the University of Wisconsin, you can query the database I am hosting on the Geography Department’s server.&lt;/p&gt;

&lt;h3 id=&quot;setting-up-the-database&quot;&gt;Setting up the Database&lt;/h3&gt;

&lt;p&gt;To have a data service, you need a data base.  The database can use any RMDBS you want (MySQL, PostGres, SQLite, MongoDB).  You should think about the system you want to use and the schema you want to implement before you start.  That’s a theme for another CLES, or Qunying’s Geography 576.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Install the Database Management System&lt;/li&gt;
  &lt;li&gt;Create user roles and privileges&lt;/li&gt;
  &lt;li&gt;Think about a relational table structure that makes sense for your data&lt;/li&gt;
  &lt;li&gt;Implement the schema&lt;/li&gt;
  &lt;li&gt;Add data (may require massaging your data, and often requires a script)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;setting-up-the-bridge&quot;&gt;Setting up the Bridge&lt;/h3&gt;
&lt;p&gt;The second component of the data service is the ‘bridge’, or what other people might call an API or application backend. I like the term bridge, because it makes clear its role as linking the database and the application client.  Call it what you like. I like to use Node.js for my bridge programs, here are some reasons: it’s fast, it’s popular, and it’s JavaScript. I find it a lot easier to write the full stack in a single language than going back and forth between different scripting languages. That being said, there are lots of other choices for language: Python, ASP, .NET, PHP.  I’ve written similar programs in both python and PHP and it’s not that different. Choose what you’re comfortable with.&lt;/p&gt;

&lt;p&gt;If you do go with node, here are the major steps (detailed below) that lead to a finished data service:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Install Node (https://nodejs.org/en/)&lt;/li&gt;
  &lt;li&gt;Create a new application using &lt;code class=&quot;highlighter-rouge&quot;&gt;npm init&lt;/code&gt; (or clone my repo)&lt;/li&gt;
  &lt;li&gt;Set up the application in &lt;code class=&quot;highlighter-rouge&quot;&gt;app.js&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Write controllers for each endpoint&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;creating-a-new-node-application&quot;&gt;Creating a new Node Application&lt;/h3&gt;
&lt;p&gt;All Node programs are also node packages, which are managed by the node package manager (&lt;code class=&quot;highlighter-rouge&quot;&gt;npm&lt;/code&gt;).  There’s a handy wizard that will guide you through the creation of a new package in &lt;code class=&quot;highlighter-rouge&quot;&gt;npm init&lt;/code&gt;.  Our program (and yours probably, if it does anything interesting) uses other people’s libraries.  There are node libraries for everything: compression, password hashing, file system access, you name it.  When in doubt, use a module, don’t try to write it yourself.  Once we’ve created the new package, we tell it about the libraries we require (these are called dependencies), by adding a section in &lt;code class=&quot;highlighter-rouge&quot;&gt;package.json&lt;/code&gt;.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Initialize new project with &lt;code class=&quot;highlighter-rouge&quot;&gt;npm init&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Follow prompts on terminal screen to initialize new application&lt;/li&gt;
  &lt;li&gt;Open &lt;code class=&quot;highlighter-rouge&quot;&gt;package.json&lt;/code&gt; with your favorite text editor.&lt;/li&gt;
  &lt;li&gt;Add the following to include dependencies:&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;&lt;span class=&quot;w&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;dependencies&quot;&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;express&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;latest&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;body-parser&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;latest&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;pg&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;latest&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;pg-promise&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;latest&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;This installs the following libraries:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;express&lt;/code&gt;: Web framework&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;body-parser&lt;/code&gt;: For getting query data&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;pg&lt;/code&gt;: Postgres bindings&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;pg-promise&lt;/code&gt;: Postgres bindings with better syntax&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Run the command &lt;code class=&quot;highlighter-rouge&quot;&gt;npm install&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;You’re now able to run your application. If you (at any time) need to add more dependencies, add them to the section you just created.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;set-up-the-application&quot;&gt;Set up the application&lt;/h3&gt;
&lt;p&gt;The bridge application will do the actual work on parsing user input values, writing SQL, executing queries, receiving database output, and packaging it up for the client.  Everything will be done in JavaScript functions, and will live in &lt;code class=&quot;highlighter-rouge&quot;&gt;app.js&lt;/code&gt;. (If you write a complex service, or use a tool like &lt;a href=&quot;http://swagger.io/&quot;&gt;Swagger&lt;/a&gt;, you may put pieces of your code into different files.  Not now.) Key parts of this file include importing libraries, connecting to the database using your credentials, and writing function for each service endpoint (next section).&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Create a file called &lt;code class=&quot;highlighter-rouge&quot;&gt;app.js&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Import the required modules (this part is a lot like python) by creating a new variable with its name:&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-javascript&quot; data-lang=&quot;javascript&quot;&gt;    &lt;span class=&quot;kd&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;express&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;require&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&#39;express&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;Create a connection to the database.
    &lt;ul&gt;
      &lt;li&gt;For this, I like to use a function, so it can be reused. For example:&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-javascript&quot; data-lang=&quot;javascript&quot;&gt;    &lt;span class=&quot;nx&quot;&gt;createDBConnection&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(){&lt;/span&gt;
      &lt;span class=&quot;c1&quot;&gt;//returns a database connection&lt;/span&gt;
        &lt;span class=&quot;kd&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;cn&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;host&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;yourHost&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;port&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5432&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;//default&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;database&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;yourDBName&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;user&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;yourUsername&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;password&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;yourPassword&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;};&lt;/span&gt;
    &lt;span class=&quot;kd&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;db&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;pgp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;cn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;//do the connection using pg-promise library&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;db&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;Write endpoint functions.&lt;/li&gt;
  &lt;li&gt;Start your application. Tell the server to listen for incoming client requests. I like to develop on ports 8000 and 8080.  Apache (web server) uses 80 (default in web browsers).&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-javascript&quot; data-lang=&quot;javascript&quot;&gt;    &lt;span class=&quot;nx&quot;&gt;app&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;listen&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8080&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;function&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
      &lt;span class=&quot;nx&quot;&gt;console&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&#39;Started application.&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;
    &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h3 id=&quot;writing-endpoint-functions&quot;&gt;Writing endpoint functions&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;In the body of your &lt;code class=&quot;highlighter-rouge&quot;&gt;app.js&lt;/code&gt; file, write a function for each endpoint you want to support. For now, we’ll just focus on &lt;code class=&quot;highlighter-rouge&quot;&gt;GET&lt;/code&gt; requests, but you can do other verbs as well (most importantly, you can add data to the database with &lt;code class=&quot;highlighter-rouge&quot;&gt;POST&lt;/code&gt; requests). Also, for now we’ll look at getting data from the query string, but you can also get data passed in via the body of the request if you want that too…&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If we want to give a user a lost of the flights (day, time, departure city, arrival city, delay amount, we might want to have an endpoint called &lt;code class=&quot;highlighter-rouge&quot;&gt;flights&lt;/code&gt; that a user would access by going to &lt;code class=&quot;highlighter-rouge&quot;&gt;https://my.website.com/api/flights&lt;/code&gt;.  That way the user knows for sure she’s getting the flights.  It’s a good idea to organize your endpoints into logical groups of what they return.&lt;/p&gt;

&lt;p&gt;However, our user might not want all the flights, rather she wants to query for those from or to a particular city, or to limit the number of results coming back from the database.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Start of by writing a function like this, for a &lt;code class=&quot;highlighter-rouge&quot;&gt;GET&lt;/code&gt; request to the &lt;code class=&quot;highlighter-rouge&quot;&gt;/flights&lt;/code&gt; service.&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-javascript&quot; data-lang=&quot;javascript&quot;&gt;  &lt;span class=&quot;nx&quot;&gt;app&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&#39;/flights&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;function&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;req&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;res&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;//this function is the endpoint for the flight data&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;//do step two here&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;//do step three here&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;//do step four here&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;//do step five here&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;

  &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;Inside of the function body, parse the user input – how are they searching or filtering:&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-javascript&quot; data-lang=&quot;javascript&quot;&gt;  &lt;span class=&quot;c1&quot;&gt;//get query parameters&lt;/span&gt;
  &lt;span class=&quot;kd&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;originCity&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;req&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;query&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;originCity&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;||&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;null&lt;/span&gt;
  &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Include one variable for each filter/query parameter/argument you want in your endpoint. This gives you the value of that parameter, if it’s in the query string, otherwise, you get a &lt;code class=&quot;highlighter-rouge&quot;&gt;null&lt;/code&gt;.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Write a SQL query, using the value other the parameter(s) given by the user. The SQL here is arbitrary – you can do anything you might want to do in PGAdmin or psql.  Joins, views, selects, deletes – it’s all on the table. Pass user parameter values in via the &lt;code class=&quot;highlighter-rouge&quot;&gt;${variableName}&lt;/code&gt; syntax, or see the &lt;a href=&quot;https://github.com/vitaly-t/pg-promise&quot;&gt;pg-promise&lt;/a&gt; library docs.&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sql&quot; data-lang=&quot;sql&quot;&gt;  &lt;span class=&quot;k&quot;&gt;sql&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;&quot;SELECT * FROM flightdelays &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;\&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;
    WHERE 1=1
    AND (${origin} IS NULL or flightdelays.origin = ${origin})&quot;&lt;/span&gt;
  &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;This syntax ensures that all results will be given back to the user in the case that no origin city is specified, and is extremely helpful for API building.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Execute the query, using the pg-promise functions. This happens asynchronously, so be prepared. You can make an object with the query values first, if that helps you think through what you’re passing to the query.&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-javascript&quot; data-lang=&quot;javascript&quot;&gt;  &lt;span class=&quot;nx&quot;&gt;db&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;any&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;sql&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;origin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;originCity&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;then&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kd&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
      &lt;span class=&quot;c1&quot;&gt;//happens on success&lt;/span&gt;
      &lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;catch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kd&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;err&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
      &lt;span class=&quot;c1&quot;&gt;//happens on error&lt;/span&gt;
      &lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;
  &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;Do any data conversions or other stuff you want. Then return the result as JSON. I like to include a timestamp, and a message that says the call succeeded.&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-javascript&quot; data-lang=&quot;javascript&quot;&gt;  &lt;span class=&quot;c1&quot;&gt;//return response to user&lt;/span&gt;
  &lt;span class=&quot;kd&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;ts&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;Date&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;toJSON&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
  &lt;span class=&quot;kd&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;resOut&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;s2&quot;&gt;&quot;success&quot;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;s2&quot;&gt;&quot;timestamp&quot;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;ts&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;data&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
  &lt;span class=&quot;nx&quot;&gt;res&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;json&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;resOut&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;//finish request by sending data back to the user&lt;/span&gt;

  &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h3 id=&quot;start-the-service&quot;&gt;Start the Service&lt;/h3&gt;
&lt;p&gt;Now that you’ve built your awesome data service, you’ll need to start it. Here’s how:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Open a new terminal/command line window&lt;/li&gt;
  &lt;li&gt;Run the command &lt;code class=&quot;highlighter-rouge&quot;&gt;node app.js&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;The server is running.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;setting-up-the-client&quot;&gt;Setting up the Client&lt;/h2&gt;
&lt;p&gt;The client can be set up as you would for any AJAX call. No special modifications are needed, you just need to know the names and data types of the values being passed in via the query string.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-javascript&quot; data-lang=&quot;javascript&quot;&gt;&lt;span class=&quot;nx&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;ajax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;localhost:8080&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;origin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&#39;PHX&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;success&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
      &lt;span class=&quot;nx&quot;&gt;console&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Got data!&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;nx&quot;&gt;console&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;
    &lt;span class=&quot;nx&quot;&gt;error&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;status&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;xhr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;error&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
      &lt;span class=&quot;nx&quot;&gt;console&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;xhr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;responseText&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;
  &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2 id=&quot;next-steps&quot;&gt;Next Steps:&lt;/h2&gt;
&lt;p&gt;If you’ve made it this far, you might want to try some more challenging tasks, such as:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Check out the annotated source in &lt;code class=&quot;highlighter-rouge&quot;&gt;app.js&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;Add a new query parameter to filter destination city.&lt;/li&gt;
  &lt;li&gt;Add a new query parameter to include only those results that have more than a certain delay (in minutes).&lt;/li&gt;
  &lt;li&gt;Add a new endpoint that lists the airports in the dataset and their states and cities.&lt;/li&gt;
  &lt;li&gt;Add a new endpoint that summarizes the delays by airport.&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Tue, 31 Jan 2017 11:00:00 -0600</pubDate>
        <link>/research/education/teaching/2017/01/31/Creating-a-data-service.html</link>
        <guid isPermaLink="true">/research/education/teaching/2017/01/31/Creating-a-data-service.html</guid>
        
        
        <category>Research</category>
        
        <category>Education</category>
        
        <category>Teaching</category>
        
      </item>
    
      <item>
        <title>Climate Data Service Released (alpha)</title>
        <description>&lt;p&gt;Several months back, I wrote about the &lt;a href=&quot;http://scottsfarley.com/research/paleoclimate/2016/06/26/the-niche-api-web-service.html&quot;&gt;niche API&lt;/a&gt; that I was putting together. I put that on hold for a hot minute while I was working on other projects, most notably, my thesis. Since the &lt;a href=&quot;http://fc.umn.edu&quot;&gt;Flyover Country&lt;/a&gt; team came to Madison in November, I’ve been pretty psyched up about it again, and have been working on it consistently for several weeks.  I’m pretty confident in it’s current version, and I hope you head over to the &lt;a href=&quot;http://grad.wisc.edu/cds&quot;&gt;live demo&lt;/a&gt; page or the &lt;a href=&quot;http://github.com/scottsfarley93/niche-api&quot;&gt;github&lt;/a&gt; to check it out.&lt;/p&gt;

&lt;p&gt;I attended the AGU annual fall meeting this week. I browsed the informatics talks and posters every day, which convinced me a data service like this is something that nobody is doing, and something that could be really special. Most folks are focused on NetCDF distribution and metadata. NetCDFs are great, and are (and will remain) and important method of disseminating climate model and earth observation data.  However, it doesn’t allow programmatic access to arbitrary geometries. OpenDAP and other protocols let you extract some subsets of a remote NetCDF, but you’ve still got to deal with a grid object on the client end.  Putting data in a more structured database (or even unstructured – there’s potential for a graph/nosql database implementation here) allows clients to request portions of single or multiple bands along sets of points, lines, or polygons.  It opens the potential for statistics (mean, median, variance, etc) within a geometry for a climate variable, allowing you to aggregate over space or time. Finally, it makes life way easier on the client side: rather than having to manipulate a grid (whole or subset), the client can consume exactly what they want.  For example, plotting time series, histograms, or environmental space plots in-browser with javascript.&lt;/p&gt;

&lt;p&gt;I think that (with a lot more work) this type of service could fundamentally change the way that people interact with climate model data.&lt;/p&gt;

&lt;p&gt;I think that important things to take care of now are standardizing the variables and source metadata to make my system consistent with leading metadata standards.  Particularly, I think I’ll work to conform my variable naming conventions to the NetCDF-CF metadata standard for variable names, and use the NASA Global Change Master Directory standards for temporal and spatial resolution conventions. I’m still looking around for climate model metadata standards, but I want to be able to differentiate the forcing scenarios, model versions, and resolutions.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://goring.org&quot;&gt;Simon Goring&lt;/a&gt; and I spent some time talking about interesting projects that could demonstrate the utility of this project. I’m not used to thinking of things in terms of papers that could be written, but it seems like there’s at least one that could be viable. I tend to like to think of things in terms of their utility to the general community – not exactly the way of the world in academia. Some potential things that we came up with:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Correlate climate reconstructions build using transfer functions from data in the Neotoma Database with climate model output. This would simplify the process of both building the transfer function (you could hit the data service for modern climate) as well as for comparison against models (you could hit the exact space-time locations).  We could then build a map of correlation between GCM-modeled climates and pollen-reconstructed climates.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Understand point-specific uncertainty in climate model projections to the end of the century. We know there is significant uncertainty both in the scenario (how much CO_2 in the atmosphere) and in the individual dynamics of the models. The climate data service could help to develop an understanding of inter-model variability at specific points.  For example, given business as usual (e.g., RCP8.5), what are the range of possible climates that Paris might experience in 2050, 2075, and 2100. Again, such an analysis would be possible without the development of my API, but it would enable public, programmatic consumption and, perhaps, promote broader public understanding.  For example, what if you could serve a visualization web-app where uncertainty ranges for any user-entered point? That’d be cool.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;There’s so much more I’d like to do on this.  Particularly, on the top of my list are:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Redesign temporal data model. Not all data will have the same resolution, and not all will have 1950 as an appropriate benchmark time. Develop a new temporal data model so that the system can handle both past and future data, as well as data that only represents a single point in time.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Ingest more models. Currently, I’ve only got the Lorenz et al. downscaled CCSM 3 data from North America. I’d like to ingest at least one more data source in the near future as a proof of concept for model inter-comparison.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Source, model, and variable metadata development. Enhance metadata so that variables and sources are well defined and can be programmatically consumed.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Develop an automated ingestion pipeline.  Allow authenticated users (me) to upload raw data (NetCDFs, raster grids, etc), convert them to the proper format, then ingest the data and the metadata into the database.  This could vastly improve the speed at which I added to the database, since everything is done by interactively now. It requires a lot of complex geoprocessing on the server though – I don’t know if the geography department will be okay with that…&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;If this sounds interesting to you, let me know! If you want to consume my data service, that’d be awesome. I’d be happy to modify the existing returns to fit your needs. Here are up-to-date &lt;a href=&quot;http://grad.geography.wisc.edu/cds/docs&quot;&gt;docs&lt;/a&gt; with examples. If you want to help develop, that’d be awesome too. I’m super excited about this project, but pushing the limits of what I know. Send me an &lt;a href=&quot;&amp;#109;&amp;#097;&amp;#105;&amp;#108;&amp;#116;&amp;#111;:&amp;#115;&amp;#102;&amp;#097;&amp;#114;&amp;#108;&amp;#101;&amp;#121;&amp;#050;&amp;#064;&amp;#119;&amp;#105;&amp;#115;&amp;#099;&amp;#046;&amp;#101;&amp;#100;&amp;#117;&quot;&gt;email&lt;/a&gt; – let’s talk.&lt;/p&gt;
</description>
        <pubDate>Fri, 16 Dec 2016 07:22:52 -0600</pubDate>
        <link>/research/data/2016/12/16/Climate-Data-Service.html</link>
        <guid isPermaLink="true">/research/data/2016/12/16/Climate-Data-Service.html</guid>
        
        
        <category>Research</category>
        
        <category>Data</category>
        
      </item>
    
      <item>
        <title>2016 NACIS Dynamic Mapping Award Win!</title>
        <description>&lt;p&gt;I’m psyched to put out that a map that I worked on last spring won the North American Cartographic Society Best Dynamic Map award at their annual conference a couple weeks back.  It was a blast working with Starr and Meghan on this project and well worth the late nights. Looking forward to more interactive mapping coming up.  If you’ve got something you want to map – let me know!&lt;/p&gt;

&lt;p&gt;Read more about the application &lt;a href=&quot;http://nacis.org/awards/2016-winner-wooden-ships/&quot;&gt;here&lt;/a&gt; or explore it here.&lt;/p&gt;

&lt;iframe src=&quot;http://wooden-ships.github.io/&quot; style=&quot;height: 1000px; width:100%&quot; /&gt;

</description>
        <pubDate>Thu, 03 Nov 2016 09:22:52 -0500</pubDate>
        <link>/tutorial/2016/11/03/Nacis-Award.html</link>
        <guid isPermaLink="true">/tutorial/2016/11/03/Nacis-Award.html</guid>
        
        
        <category>Tutorial</category>
        
      </item>
    
      <item>
        <title>Occurrences in Neotoma: 18,900,000 and counting...</title>
        <description>&lt;p&gt;A few weeks ago, I posted about Big Data in the field of ecology, specifically biodiversity informatics, where I looked at the holdings of the Neotoma Paleoecological Database and the Global Biodiversity Information Facility (GBIF). I made some comparisons between the two databases, though the units of scale were different. In Neotoma, I used the number of &lt;em&gt;datasets&lt;/em&gt; that had been submitted, while for GBIF, I commented on the number of &lt;em&gt;occurrences&lt;/em&gt;.  These are two fundamentally different units, so I set out to resolve the issue and find out how many occurrences there are in Neotoma.&lt;/p&gt;

&lt;p&gt;Counting every single taxa in every single level in Neotoma, there are over 18.9 million occurrence records – identifications of a single type at a single space-time locus.&lt;/p&gt;

&lt;p&gt;To be exact, there are 18,903,236 occurrence records, with a steeply increasing trend over the last several months. I suspect we’ll be up over 20 million by the middle of 2017.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/bigData/Neotoma_Occurrences.png&quot; alt=&quot;NeotomaOccurrenceRecords&quot; /&gt;&lt;/p&gt;
</description>
        <pubDate>Sat, 22 Oct 2016 09:22:52 -0500</pubDate>
        <link>/tutorial/2016/10/22/18-Million-Neotoma-Occurrences.html</link>
        <guid isPermaLink="true">/tutorial/2016/10/22/18-Million-Neotoma-Occurrences.html</guid>
        
        
        <category>Tutorial</category>
        
      </item>
    
      <item>
        <title>PhD Preview</title>
        <description>&lt;p&gt;I’ve been thinking a lot about my PhD research, even as I’ve been working 12 hours a day to finish my master’s thesis. If you were recently thinking &lt;em&gt;“I wonder what Scott will be doing in Wisconsin for the next 3-6 years”&lt;/em&gt;, wonder no more. This is the Cliff Notes version, expect a 150-200 page version in early 2021.&lt;/p&gt;

&lt;p&gt;Contemporary understanding of past land cover dynamics comes primarily from expert interpretation of paleoenvironmental proxies, such as fossil pollen or vertebrate macrofossils. My dissertation research will focus on the link between the paleoecological proxy data, particularly fossil pollen, and the drivers of land use change by developing a hierarchical Bayesian model that calibrates dense networks of modern pollen samples with MODIS remote sensing data.  Once calibrated, the model will be used to develop a gridded dataset that reconstructs tree cover percentage and relative composition of broadleaf and needleleaf plant types at 500-year intervals since the Last Glacial Maximum (22kya). The reconstructions can then be used to to identify and test the drivers of regional and global scale land cover shifts, in response to climate and other forcings. I plan to test the impact of vegetation change during the late Pleistocene to draw inference about megafaunal decline and extinction. My work bridges a key gap by providing a mechanism to propagate our understanding of modern land cover change, in the form of remote sensing data products, back through time while estimating model uncertainty.  Using land cover dynamics from the recent geologic past can help to forecast how land cover and biodiversity will change when exposed to 21st century climatic and anthropogenic forcings.&lt;/p&gt;

&lt;h4 id=&quot;research-plan&quot;&gt;Research Plan&lt;/h4&gt;
&lt;p&gt;I plan to split my research into three phases, each lasting approximately one year.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Model Development and Parameterization: During this phase, I will develop a spatiotemporal Bayesian hierarchical regression model to reconstruct past land cover, focusing on model development, calibration, and parameterization.  I will use the Bayesian framework to assimilate modern land cover datasets, derived from Moderate Resolution Imaging Spectroradiometer (MODIS) satellite data and modern pollen surface samples from the Neotoma Paleoecological Database and the North American Modern Pollen Database. The MODIS data is an gridded product describing the relative proportion of broad leaved, needle leaved, coniferous, and deciduous tree cover, and non-arboreal land cover in each 5’ grid cell. The modern pollen surface samples, which record the relative abundance of taxa in the local biota, will be used to regress the remote sensing data onto the sedimentary record, allowing me to calibrate a predictive model to infer land cover at 500-year intervals from the Last Glacial Maximum to the present in North America. The model will specifically incorporate spatial and temporal uncertainty in pollen source area and dispersal kernels, temporal and dating uncertainty in the fossil record, and uncertainties inherent in the classification process used by MODIS. Specific care will be taken to develop a robust model that can be applied both the relative homogeneous eastern forests and the highly complex and mountainous vegetation mosaics in the western areas of the continent.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Model Assessment and Human Impacts: I will also look specifically at the model’s reconstructions of areas of known human development to determine the model’s success in capturing human landscape-scale modification processes, such as burning and agriculture.  I will work with the archeological literature and paleoenvironmental syntheses, such as those using fossil charcoal, that document human landscape alteration, to compare know areas of impact with the model’s estimates.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Late-Pleistocene Megafauna Extinction: I will demonstrate the usefulness of the model by applying its output to an often-studied problem in paleoecological change: determining the key drivers of megafuana extinction during the late Pleistocene.  Models of megafauna decline are currently often only informed by climate model output and expert-inferred climate and vegetation characteristics from paleoecological proxies, such as fossil pollen. I will use the land cover reconstructions I develop to assess the degree to which land cover change was a driver of fuanal decline.  Using species distribution modeling techniques, I will evaluate the degree to which vegetation and climate contribute to the decline individually and when they are allowed to interact. This phase will demonstrate the model’s application and reconstructions ability to be incorporated as an input into other models, and provide new insight into a lingering question in global change research.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;importance&quot;&gt;Importance&lt;/h4&gt;
&lt;p&gt;In addition to contributing to scientific understanding of human-induced change and faunal declines, the techniques and datasets produced in the course of this research are essential for improving the estimates of carbon sequestration and warming during the 21st century produced by ecosystem and global circulation climate models. The advantages of Bayesian hierarchical models for reconstructing past landscapes have been demonstrated, though they have yet to be applied to the continental scale and to the problem of land cover in North America, though other techniques have been applied to European land cover during the Holocene. Biogeophysical feedback cycles, particularly the carbon cycle, can have considerable effects, though their operational scale is too small to be effectively captured by the still relatively rough resolution of global earth system models, so independent reconstructions are of high priority to climate modelers. Additionally, hypotheses about carbon sequestration and climate change mitigation strategies cannot be validated without further understanding of past land cover dynamics.&lt;/p&gt;
</description>
        <pubDate>Mon, 19 Sep 2016 09:22:52 -0500</pubDate>
        <link>/research/2016/09/19/PHD-Preview.html</link>
        <guid isPermaLink="true">/research/2016/09/19/PHD-Preview.html</guid>
        
        
        <category>Research</category>
        
      </item>
    
      <item>
        <title>William&#39;s Lab Github Practical</title>
        <description>&lt;h3 id=&quot;objectives&quot;&gt;Objectives&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;Introduce Git and Github: what are they, what’s the difference, and how can they be useful to your work in the lab?&lt;/li&gt;
  &lt;li&gt;Create your first repository and practice the workflow of committing and pulling/pushing files.&lt;/li&gt;
  &lt;li&gt;Talk about some of the more advanced features of tools: why might you need to use them, particularly in terms of collaboration.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;git-and-github&quot;&gt;Git and Github&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Git&lt;/strong&gt; is a open source version control system, a tool that manages the changes of files in a project. Each revision is associated with a timestamp and the person making the change, making it easy to revert back to previous versions of the file if you make a mistake or accidentally break something.  Version control systems are most commonly used for source code management, most any type of files can be managed using a VCS. There are other version control systems on the market today (e.g., Mercurial, Subversion); Git is among the most popular today. Git operates by having distributed project ‘repositories’ that store your code.  Any server can be a Git server.  For example, the William’s lab server could be a Git server if we set it up that way.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Github&lt;/strong&gt; is a company that provides web-based Git repository hosting.  Github manages the servers used to store your project repositories, supports all features of git like branches, merges, and commits, as well as adding additional tools that facilitate project development and collaboration like wiki hosting and issue tracking. Github is a private company valued at over $2 billion with about 20 million &lt;a href=&quot;https://www.quora.com/How-many-users-does-GitHub-have&quot;&gt;users&lt;/a&gt; and nearly 40 million different projects. Before you trust it completely remember (1) it is a company that has motives other than just backing up your code and (2) if you choose to use GitHub and they do something bad or there is a systemwide failure, there are going to be 20 million other people just, if not more, as upset as your are. There are other git hosting platforms as well: Bitbucket, Gitlab, etc.&lt;/p&gt;

&lt;h3 id=&quot;getting-started&quot;&gt;Getting Started&lt;/h3&gt;

&lt;h4 id=&quot;create-an-account&quot;&gt;Create an account&lt;/h4&gt;

&lt;p&gt;To use Github, you need to create an account.  The good news: basic accounts are free to everybody. People with a basic account can create an unlimited of public repositories (can bee seen by anyone). If you have an academic (.edu) email address, you can get an unlimited number of private repositories as well by signing up &lt;a href=&quot;https://education.github.com/pack&quot;&gt;here&lt;/a&gt;. Private repositories let you control who sees your code – it’s hidden from the general public.  In both public and private repositories you can control who changes your code.&lt;/p&gt;

&lt;p&gt;To create an account, go to &lt;a href=&quot;https://github.com/join&quot;&gt;https://github.com/join&lt;/a&gt; and fill out the fields.  Make sure to use you &lt;code class=&quot;highlighter-rouge&quot;&gt;@wisc.edu&lt;/code&gt; email address.  Keep in mind that your user name will be your public name that identifies you on the platform and will be attached to everything you do there. Also, you’ll need to type in your username from time to time, so don’t make it &lt;em&gt;alongandannoyingtotypusername&lt;/em&gt;.&lt;/p&gt;

&lt;h4 id=&quot;getting-git-on-your-computer&quot;&gt;Getting Git on Your Computer&lt;/h4&gt;
&lt;p&gt;To use Git on your computer, you need to install it and configure it to work with Github servers. This content was borrowed from &lt;a href=&quot;https://help.github.com/articles/set-up-git/&quot;&gt;the Git Docs&lt;/a&gt;.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Download the latest version of Git from the &lt;a href=&quot;https://git-scm.com/downloads&quot;&gt;Git Website&lt;/a&gt;. Make sure to get the distribution that corresponds to your operating system, i.e., if you have a Windows machine, don’t opt for the Mac download.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Install Git by following the instructions on the download.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Open the &lt;code class=&quot;highlighter-rouge&quot;&gt;terminal&lt;/code&gt; on your computer.  In windows this is the &lt;code class=&quot;highlighter-rouge&quot;&gt;cmd&lt;/code&gt; shell.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Tell Git your name so your changes will be attributed to your Github account:&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;  git config --global user.name &lt;span class=&quot;s2&quot;&gt;&quot;YOUR NAME&quot;&lt;/span&gt;
  &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;If the terminal does not recognize &lt;code class=&quot;highlighter-rouge&quot;&gt;git&lt;/code&gt; as a command, you might not have installed it correctly, return to step 1.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Tell Git your email address associated with your account.  Make sure to use the same email your signed up with, or things can get funky later on.&lt;/li&gt;
&lt;/ol&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;  git config --global user.email &lt;span class=&quot;s2&quot;&gt;&quot;YOUR EMAIL ADDRESS&quot;&lt;/span&gt;
  &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;From here on out, the changes you make to your project and other people’s projects (if you’re collaborating) will be attributed to this email address and username.&lt;/p&gt;

&lt;h4 id=&quot;create-a-repository&quot;&gt;Create a repository&lt;/h4&gt;

&lt;p&gt;A repository is where your project lives. From the github &lt;a href=&quot;https://help.github.com/articles/github-glossary/#repository&quot;&gt;documentation&lt;/a&gt;:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;“A repository is the most basic element of GitHub. They’re easiest to imagine as a project’s folder. A repository contains all of
  the project files (including documentation), and stores each file’s revision history. Repositories can have multiple collaborators and can be either public or private.”&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Github is not just a way to version control your code, it’s also a way to organize your work.  A repository should contain all of the code, files, documentation, etc needed for a project.  For example, I have one repository for my thesis, a separate one for &lt;a href=&quot;https://github.com/scottsfarley93/IceAgeMapper&quot;&gt;Ice Age Mapper&lt;/a&gt;, and another one for &lt;a href=&quot;https://github.com/scottsfarley93/scottsfarley93.github.io&quot;&gt;my blog posts&lt;/a&gt;. Each one has everything it needs for someone to download it and work with that project on their own computer, but nothing extra that might span over to another project.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;There are several ways to create a new repository:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Click on the ‘+’ button in the top right of any page, then select ‘New repository’.
 &lt;img src=&quot;/assets/github/create_2.png&quot; alt=&quot;Create1&quot; /&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;On your home page, click the green ‘New’ button.
 &lt;img src=&quot;/assets/github/create_1.png&quot; alt=&quot;Create2&quot; /&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Go to &lt;a href=&quot;https://github.com/new&quot;&gt;https://github.com/new&lt;/a&gt;.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Create a repository from your desktop.  This is a bit more involved.  See &lt;a href=&quot;https://help.github.com/articles/adding-an-existing-project-to-github-using-the-command-line/&quot;&gt;this tutorial&lt;/a&gt;.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Give your repository a name.  This name should be short, yet descriptive of your project.  You’ll have to type this name every time you want to access your project so short and sweet is ideal.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Add a description of the project.  This is a useful place to describe a bit more about your project – particularly important if you’re sharing your repo with other collaborators.  Click ‘Initialize this project with a README’. This will give you a &lt;code class=&quot;highlighter-rouge&quot;&gt;README.md&lt;/code&gt; file where you can describe your project in detail. You can populate this file with more details later.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Choose whether to make your project public or private.  I would recommend public repos for most projects, unless they contain sensitive code/information, stuff for publication, or other content you don’t want everyone in the world to see.  A major part of working with code and on github in particular is collaboration and openness with projects. I currently have only two private repositories: (1) labs and solutions for Geography 378 and (2) my thesis, because it contains details about the database I’m pushing data into.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Choose whether to add a &lt;code class=&quot;highlighter-rouge&quot;&gt;.gitignore&lt;/code&gt; file and/or a license file.  The &lt;code class=&quot;highlighter-rouge&quot;&gt;.gitignore&lt;/code&gt; file tells the git system not to track specific file types.  A &lt;code class=&quot;highlighter-rouge&quot;&gt;.gitignore&lt;/code&gt; file can be configured to tell git not to track any type of file.  If you click the box at this point, it will pre-populate a gitignore for you with a list of files that are often ignored when working in a particular language.  For example, if you’re working in R, selecting an &lt;code class=&quot;highlighter-rouge&quot;&gt;R&lt;/code&gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;.gitignore&lt;/code&gt; will stop the tracking of .Rdata (your workspace), .Rhistory (your command history), Rstudio files, caches, tempory directories, and files associated with converting markdown to documentation.  See &lt;a href=&quot;https://github.com/github/gitignore&quot;&gt;this repository&lt;/a&gt; for a list of all the options and the files they ignore. You can add to this file later in the course of your project too.&lt;/p&gt;

    &lt;p&gt;A license file tells users of your code how it may be used.  If you’ve chosen to use a public repository, you’ve already committed to some degree of openness in content use. The license file will discriminate between things like commercial use, totally unrestricted use, etc. Github produced &lt;a href=&quot;http://choosealicense.com/&quot;&gt;this website&lt;/a&gt; if you need to choose a license but don’t know which one you need.  I don’t usually license my repos.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;setting-up-your-repository&quot;&gt;Setting up your repository&lt;/h4&gt;

&lt;p&gt;Now you have a new repository, it’s currently living only on the Github servers. It’s time to set it up on your computer. There are a couple ways to do this.  I’ll focus on using command line, because it provides the most flexibility, though it can be a little intimidating at first. Make sure you have git installed on your computer before starting. I have a mac, so commands here are POSIX (mac and linux) specific, but the same concepts apply to git shell in windows. Kevin uses git on windows, so he can help answer your questions here.&lt;/p&gt;

&lt;p&gt;We’re first going to ‘clone’ the repository into a folder of your choosing.  It will create a new folder, so cloning into your documents folder will make a folder in &lt;code class=&quot;highlighter-rouge&quot;&gt;documents/yourRepo&lt;/code&gt;. Cloning will load all of the files contained in the github repository onto your local machine. The clone URL is the URL of your project, e.g., &lt;a href=&quot;https://github.com/scottsfarley93/IceAgeMapper&quot;&gt;https://github.com/scottsfarley93/IceAgeMapper&lt;/a&gt;. To change folders in the command line shell, you will use the &lt;code class=&quot;highlighter-rouge&quot;&gt;cd&lt;/code&gt; command.  &lt;code class=&quot;highlighter-rouge&quot;&gt;cd ..&lt;/code&gt; navigates up in the directory structure, &lt;code class=&quot;highlighter-rouge&quot;&gt;cd [directoryName]&lt;/code&gt; switches into a directory. &lt;code class=&quot;highlighter-rouge&quot;&gt;ls&lt;/code&gt; lists the content of a directory, and can be helpful to figure out where you are.  Once you’ve found a place you want to clone into, clone the repository:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;  git clone https://github.com/[YOUR USERNAME]/[YOUR REPOSITORY NAME]
  &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Your repository is now set up as a repository folder on your computer and any changes we make &lt;em&gt;inside&lt;/em&gt; of that folder, will be tracked.  At specific points in time, we’ll tell Git that we’ve made a suitable number of changes and want to mark that as a waypoint in our project history, using commits.&lt;/p&gt;

&lt;h4 id=&quot;making-changes&quot;&gt;Making Changes&lt;/h4&gt;
&lt;p&gt;Github tracks things called Diffs – a record of what was added, deleted, or changed inside of every file in your repo, a lot like the track changes functionality in MS Word. Git is different that Word, though, because you can revert back to previous timestamps. To mark a specific timestamp as a place you might want to come back to, you &lt;strong&gt;commit&lt;/strong&gt; your project. Committing is kind of like adding a waypoint to your project – each file is marked at its current configuration at this point in time. First, we’re going to tell Git which files to add to the commit, then we’re going to do the commit, identifying it by a short message. Finally, we will push our local changes to the Github server. Before committing, open a terminal and navigate (using &lt;code class=&quot;highlighter-rouge&quot;&gt;cd&lt;/code&gt;) to inside of your repository folder.&lt;/p&gt;

&lt;p&gt;1. Add files to the commit:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;  git add --all
  &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;It is possible to only add specific files to your commit, but I nearly always just tell git to track all of the files inside of my project repository. If you don’t want specific files to be tracked, you can add them to your &lt;code class=&quot;highlighter-rouge&quot;&gt;.gitignore&lt;/code&gt; file.&lt;/p&gt;

&lt;p&gt;2. Make the commit&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;  git commit -m &lt;span class=&quot;s2&quot;&gt;&quot;THIS IS A COMMIT MESSAGE&quot;&lt;/span&gt;
  &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Your commit messages should be short, easy to identify messages about what you changed since your last commit.  Traditional commit messages are things like “Added feature XYZ”, “Fixed bug ABC”, “Changed QRS to MNO”, etc. It is important to clearly identify your commits so that if you need to revert back to a previous version, you know which version to use.&lt;/p&gt;

&lt;p&gt;3. Push your changes to the Github server&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;  git push &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;origin branch]
  &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;This step takes all of the commits that we’ve made on the local computer and pushes them out to the Github server.  It is not absolutely necessary to push after every commit, but I usually do – that way it’s backed up remotely on the Github server. Depending on your project configuration and if you’re working on a branch that’s not the default (more on that later) you may need to add the &lt;code class=&quot;highlighter-rouge&quot;&gt;origin branch&lt;/code&gt; at the end of the line to properly push.&lt;/p&gt;

&lt;h4 id=&quot;making-changes-an-example&quot;&gt;Making Changes: An Example&lt;/h4&gt;
&lt;p&gt;To show you how Github keeps track of things, we’re going to work through a short example.&lt;/p&gt;

&lt;p&gt;1. Open R or RStudio. Open a new RScript. We’re first going to define a variable &lt;code class=&quot;highlighter-rouge&quot;&gt;x&lt;/code&gt; equal to 1. If you don’t like R, just open a text file and play along using your own short content examples.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;    &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;## define x to be one
&lt;/span&gt;    &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Save the file into your project repo folder.&lt;/p&gt;

&lt;p&gt;2. Let’s commit the file now.  It’s traditional on the first commit you make on your project to give it the message &lt;code class=&quot;highlighter-rouge&quot;&gt;&quot;Initial Commit&quot;&lt;/code&gt;.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;    git add --all
    git commit -m &lt;span class=&quot;s2&quot;&gt;&quot;Initial Commit&quot;&lt;/span&gt;
    git push
    &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;You should see something like:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;    Scotts-MacBook-Air:Williams-Lab-Github-Practical scottsfarley&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;git commit -m &lt;span class=&quot;s2&quot;&gt;&quot;Initial Commit&quot;&lt;/span&gt;
  &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;master 6fef42f] Initial Commit
   1 file changed, 1 insertion&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;+&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
   create mode 100644 myTrackedFile.R
  Scotts-MacBook-Air:Williams-Lab-Github-Practical scottsfarley&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;git push
  warning: push.default is &lt;span class=&quot;nb&quot;&gt;unset&lt;/span&gt;; its implicit value has changed &lt;span class=&quot;k&quot;&gt;in
  &lt;/span&gt;Git 2.0 from &lt;span class=&quot;s1&quot;&gt;&#39;matching&#39;&lt;/span&gt; to &lt;span class=&quot;s1&quot;&gt;&#39;simple&#39;&lt;/span&gt;. To squelch this message
  and maintain the traditional behavior, use:

    git config --global push.default matching

  To squelch this message and adopt the new behavior now, use:

    git config --global push.default simple

  When push.default is &lt;span class=&quot;nb&quot;&gt;set &lt;/span&gt;to &lt;span class=&quot;s1&quot;&gt;&#39;matching&#39;&lt;/span&gt;, git will push &lt;span class=&quot;nb&quot;&gt;local &lt;/span&gt;branches
  to the remote branches that already exist with the same name.

  Since Git 2.0, Git defaults to the more conservative &lt;span class=&quot;s1&quot;&gt;&#39;simple&#39;&lt;/span&gt;
  behavior, which only pushes the current branch to the corresponding
  remote branch that &lt;span class=&quot;s1&quot;&gt;&#39;git pull&#39;&lt;/span&gt; uses to update the current branch.

  See &lt;span class=&quot;s1&quot;&gt;&#39;git help config&#39;&lt;/span&gt; and search &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&#39;push.default&#39;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;further information.
  &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;the &lt;span class=&quot;s1&quot;&gt;&#39;simple&#39;&lt;/span&gt; mode was introduced &lt;span class=&quot;k&quot;&gt;in &lt;/span&gt;Git 1.7.11. Use the similar mode
  &lt;span class=&quot;s1&quot;&gt;&#39;current&#39;&lt;/span&gt; instead of &lt;span class=&quot;s1&quot;&gt;&#39;simple&#39;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if &lt;/span&gt;you sometimes use older versions of Git&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

  Counting objects: 3, &lt;span class=&quot;k&quot;&gt;done&lt;/span&gt;.
  Delta compression using up to 4 threads.
  Compressing objects: 100% &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;2/2&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;, &lt;span class=&quot;k&quot;&gt;done&lt;/span&gt;.
  Writing objects: 100% &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;3/3&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;, 327 bytes | 0 bytes/s, &lt;span class=&quot;k&quot;&gt;done&lt;/span&gt;.
  Total 3 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;delta 0&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;, reused 0 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;delta 0&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
  To https://github.com/scottsfarley93/Williams-Lab-Github-Practical
     80ddf92..6fef42f  master -&amp;gt; master
  &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;If you see a message that informs you that your commit was &lt;strong&gt;&lt;em&gt;rejected&lt;/em&gt;&lt;/strong&gt;, try pulling remote changes first (next section).&lt;/p&gt;

&lt;p&gt;3.  We’re really hard working grad students, so now we’ve updated our R file. Specifically, we no longer think that &lt;code class=&quot;highlighter-rouge&quot;&gt;x&lt;/code&gt; is 1, but rather 2. Also, we’ve discovered a variable &lt;code class=&quot;highlighter-rouge&quot;&gt;y&lt;/code&gt;, and it’s value is 5. In our R file, we’re going to change the value of &lt;code class=&quot;highlighter-rouge&quot;&gt;x&lt;/code&gt; and add the statement defining &lt;code class=&quot;highlighter-rouge&quot;&gt;y&lt;/code&gt;:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;  &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;5&lt;/span&gt;
  &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Save the file and commit again.&lt;/p&gt;

&lt;p&gt;4. Now, go to &lt;a href=&quot;http://github.com&quot;&gt;github.com&lt;/a&gt; and let’s look at the changes that github keeps track of and how we might use the file history.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;a. Go to your project repository page. Notice that your R file that we’ve been working on appears there now.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;b. Click on the file’s name to see more details about it.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/github/github_file.png&quot; alt=&quot;GithubFile&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;c. Click the &lt;code class=&quot;highlighter-rouge&quot;&gt;history&lt;/code&gt; button to show a summary of the commits that have been placed on that file.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/github/github_history.png&quot; alt=&quot;github_history&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;d. Notice that there are two commits on this file: the first one, when we first created it and the second one, after we added the variable &lt;code class=&quot;highlighter-rouge&quot;&gt;y&lt;/code&gt;.  Click on a commit message to see what was changed during that commit. Click on the most recent commit message.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/github/github_commit.png&quot; alt=&quot;github_commit&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;e. Here is where we can see what Github is keeping track of. In the red, we see things that we removed from the file in the commit.  In the green are lines that were added to the file during that commit. Each file is identified by a hash – the long string of letters and numbers.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;f. To look at the raw files at the point when the commit was made, go back one page to the listing of commits and click on the &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt; &amp;gt;&lt;/code&gt; button to show the file at that point in time.  You will notice when you do that is not project in its current state, but shows the state of project when you made that commit.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/github/git_older.png&quot; alt=&quot;github_older&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;pulling-remote-files&quot;&gt;Pulling remote files&lt;/h4&gt;
&lt;p&gt;One of the most powerful aspects of Github is its ability to unify your workflow across different computers. For example, Kevin is using it to merge code changes across the desktop in his office and his laptop. If you’re using it in this way, you’re going to need to update your working copy (e.g., on your laptop) with other changes you made previously (e.g., on the desktop).  To do that, we’re going to &lt;em&gt;sync&lt;/em&gt; or &lt;em&gt;pull&lt;/em&gt; changes from the Github server. &lt;strong&gt;This is different from a pull request.&lt;/strong&gt;  We’ll touch on pull requests later. The terminology here is one of the most difficult parts of github, because it conflicts with another aspect of the platform and it is different between the command line and the desktop app (if you’ve used that). Nonetheless, it’s actually pretty easy to do:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;  git pull
  &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;At this point a number of things might happen:&lt;/p&gt;

&lt;p&gt;1.  Everything you have on your local machine already reflects the most recent changes; there were not remote changes to fetch:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;  Scotts-MacBook-Air:Williams-Lab-Github-Practical scottsfarley&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;git pull
  Already up-to-date.
  &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;2.  You have remote changes, but it can be done without conflicts:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;  Scotts-MacBook-Air:Williams-Lab-Github-Practical scottsfarley&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;git pull
  remote: Counting objects: 3, &lt;span class=&quot;k&quot;&gt;done&lt;/span&gt;.
  remote: Compressing objects: 100% &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;2/2&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;, &lt;span class=&quot;k&quot;&gt;done&lt;/span&gt;.
  remote: Total 3 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;delta 0&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;, reused 0 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;delta 0&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;, pack-reused 0
  Unpacking objects: 100% &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;3/3&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;, &lt;span class=&quot;k&quot;&gt;done&lt;/span&gt;.
  From https://github.com/scottsfarley93/Williams-Lab-Github-Practical
     d988f65..1b21b38  master     -&amp;gt; origin/master
  Updating d988f65..1b21b38
  Fast-forward
   myOtherTrackedFile | 1 +
   1 file changed, 1 insertion&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;+&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
   create mode 100644 myOtherTrackedFile
  &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;3.  You have remote changes and they conflict with previous changes you have on your computer that you need to merge manually:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;  remote: Counting objects: 3, &lt;span class=&quot;k&quot;&gt;done&lt;/span&gt;.
  remote: Compressing objects: 100% &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;2/2&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;, &lt;span class=&quot;k&quot;&gt;done&lt;/span&gt;.
  remote: Total 3 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;delta 1&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;, reused 0 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;delta 0&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;, pack-reused 0
  Unpacking objects: 100% &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;3/3&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;, &lt;span class=&quot;k&quot;&gt;done&lt;/span&gt;.
  From https://github.com/scottsfarley93/Williams-Lab-Github-Practical
     7ebd809..ecf9d38  master     -&amp;gt; origin/master
  Auto-merging myTrackedFile.R
  CONFLICT &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;content&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;: Merge conflict &lt;span class=&quot;k&quot;&gt;in &lt;/span&gt;myTrackedFile.R
  Automatic merge failed; fix conflicts and &lt;span class=&quot;k&quot;&gt;then &lt;/span&gt;commit the result.
  &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;In this case, you will need to determine where your changes conflict with each other, fix them, make a new commit, and try syncing again.  If you navigate to your files, you will find something that looks like this:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;    &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;
    &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;HEAD&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;6&lt;/span&gt;
    &lt;span class=&quot;o&quot;&gt;=======&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;
    &lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ecf9d381b84656db7f08c480d5989145a5d79b7f&lt;/span&gt;

  &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;This shows you where the conflict is.  Delete the lines you don’t want, removing the commit hash strings, &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;gt;&lt;/code&gt; characters as well.  Try committing again. This is a pain in the ass.  Try to avoid having this happen.&lt;/p&gt;

&lt;p&gt;4.  You have remote changes and they conflict with previous changes on your copy, but they can be merged automatically:&lt;/p&gt;

&lt;p&gt;In this case, you will be able to merge automatically, but you will need to add a “merge message” – similar to a commit message.  A text editor will open in your terminal where you can enter this message.  Depending on your computer, one of at least two different editors will open.  On a mac, either &lt;code class=&quot;highlighter-rouge&quot;&gt;vim&lt;/code&gt; or &lt;code class=&quot;highlighter-rouge&quot;&gt;nano&lt;/code&gt; will open to do the changes. To use &lt;code class=&quot;highlighter-rouge&quot;&gt;vim&lt;/code&gt;, type your commit message then &lt;kbd&gt;:&lt;/kbd&gt; + &lt;kbd&gt;q&lt;/kbd&gt; + &lt;kbd&gt;Return/Enter&lt;/kbd&gt;.  To use nano, type your message, then &lt;kbd&gt;Control&lt;/kbd&gt; + &lt;kbd&gt;o&lt;/kbd&gt; to save your edits and &lt;kbd&gt;Control&lt;/kbd&gt; + &lt;kbd&gt;x&lt;/kbd&gt; to exit the editor. On windows, you’re on your own. You can set your default editor using command line, which would be helpful.&lt;/p&gt;

&lt;p&gt;The complexity here is for a reason – to be able to test code that comes from your collaborators before you merge it into your copy.  If you’re just working on your own project, these changes are always your own, so this merging stuff seems like overkill. But, when you’re working with one or more other people, you can’t be sure that something won’t break when you include their code.&lt;/p&gt;

&lt;h4 id=&quot;pulling-remote-files-an-example&quot;&gt;Pulling Remote Files: An Example&lt;/h4&gt;
&lt;p&gt;Github.com allows you to edit your files online, so we’re going to use that as a proxy for having two computers to work with. Note: I wouldn’t really recommend using the online platform to edit your files in (though I do it all the time).  If you’re going to make changes, just pull your repo and make changes on your local computer, or you risk having a lot of merge conflicts.&lt;/p&gt;

&lt;p&gt;1.  Go to your project repo page on github.com.&lt;/p&gt;

&lt;p&gt;2.  Create a new file, using the &lt;code class=&quot;highlighter-rouge&quot;&gt;Create new file&lt;/code&gt; button.&lt;/p&gt;

&lt;p&gt;3.  Give it a name and remember to add the file extension.&lt;/p&gt;

&lt;p&gt;4.  Type some stuff.  For this example, I’m ‘using’ R, so I’ll define a new variable &lt;code class=&quot;highlighter-rouge&quot;&gt;z&lt;/code&gt;.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;  &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;10238&lt;/span&gt;
  &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;5.  Save/commit the file. Github will automatically give you a commit message of “[Create/Update] [YOUR FILENAME]” if you’re doing changes on Github.com, so feel free to use that.&lt;/p&gt;

&lt;p&gt;6.  Return to your terminal, making sure you’re in the right project directory.&lt;/p&gt;

&lt;p&gt;7.  Pull the changes you made remotely.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;  git pull
  &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;8.  Look in the project folder.  Note that the file your created online is now stored on your local computer, the same as any other file in your project.&lt;/p&gt;

&lt;h4 id=&quot;advanced-github-features-and-scenarios&quot;&gt;Advanced Github: Features and Scenarios&lt;/h4&gt;

&lt;p&gt;One of the most confusing and intimidating aspects of Git/Github is the complicated and diverse set of features and terminology that goes along with the core concepts introduced above. Complicated and diverse, but powerful, especially when you work on long projects with multiple collaborators. Here, we will take a look at some of these concepts, when you might need them, and how they might be useful to your work.  By time time you need these constructs, you will be an expert in committing and syncing with remote repositories, so reading the docs for these things shouldn’t be as challenging. Content in the following section makes heavy use of the Github docs.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Branches&lt;/p&gt;

    &lt;p&gt;&lt;strong&gt;Scenario&lt;/strong&gt;: You’ve been working super hard all week on some super cool code stuff.  On Friday afternoon, you finally get it to work! You’re super psyched.  But, your weekly meeting with Jack is not until the following Wednesday.  Being the productive grad student that you are, you want to keep working on it over the weekend, adding a new feature.  But you also want to impress Jack with your recent developments and don’t want those to be compromised by having a feature that might break those whole thing all together. What to do?&lt;/p&gt;

    &lt;p&gt;Enter &lt;em&gt;branches&lt;/em&gt;. Branches are parallel projects, isolated from each other, on which you can develop new features, without fear that you will forever break your main code.  From the git documentation: “Branching means you diverge from the main line of development and continue to do work without messing with that main line.”  Git encourages branching early and often, even multiple times in a day.&lt;/p&gt;

    &lt;p&gt;So, in the case of our scenario, we are able to have a branch &lt;code class=&quot;highlighter-rouge&quot;&gt;master&lt;/code&gt; that contains our version that we will show Jack.  On a &lt;em&gt;separate branch&lt;/em&gt; we will have our new feature.  We can develop on our new feature without messing up the master branch. When it comes time to meet with Jack, we will just sync to the latest commit of the master branch.  After we’ve perfected our new feature, we can merge it into the master branch and make it part of the main project.&lt;/p&gt;

    &lt;p&gt;Committing and syncing on feature branches goes the exact same way as committing and syncing on the master branch.  In fact, the master branch isn’t even a special branch, it’s just created by default when you make the repository, and you probably didn’t change it.  When you’re feature is ready to go prime time, you can merge the feature back into the master branch, even if you’ve made additional changes to the master branch since splitting off on the feature branch.  Look at &lt;a href=&quot;https://www.atlassian.com/git/tutorials/using-branches/git-merge&quot;&gt;this tutorial&lt;/a&gt; for an in-depth, accessible look at branching and merging.&lt;/p&gt;

    &lt;p&gt;&lt;a href=&quot;https://www.atlassian.com/git/images/tutorials/collaborating/using-branches/01.svg&quot;&gt;&lt;img src=&quot;https://www.atlassian.com/git/images/tutorials/collaborating/using-branches/01.svg&quot; alt=&quot;git_branches&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Pull Requests&lt;/p&gt;

    &lt;p&gt;&lt;strong&gt;Scenario&lt;/strong&gt;: You’re favorite R package is lacking a function that you think would be incredibly useful to you and other users of the package. You could submit an &lt;em&gt;issue&lt;/em&gt;, but you’re smart and figured out how to code the solution by yourself. You could just keep the code to yourself, but that’s no fun. Instead, you can make a &lt;em&gt;pull request&lt;/em&gt; to have your new code merged into the R package’s github repository, and thus the R package itself. In general, a pull request is a way to merge code into an existing repo.  If you just automatically merged it, it could break existing code and that would be bad. A pull request lets the owners of the repository test and discuss proposed changes with collaborators and add additional commits before the code is merged. Once everyone has agreed that the changes are good, the pull request can be approved and the changes will be merged.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Issues&lt;/p&gt;

    &lt;p&gt;&lt;strong&gt;Scenario&lt;/strong&gt;: You’re working on a geovisualization platform for Neotoma data.  You talk with Jack, who talks with Eric Grimm and Simon and the other Neotoma folks, and you start getting emails about features that people would like or bugs that people have found while using your code.  You could just keep these emails, perhaps in a separate email folder, to remember what needs fixing or enhancing, or you could create a &lt;em&gt;Github issue&lt;/em&gt; to keep track of each feature and bug.  Issues let you make comments on feature requests and bug reports that are publicly viewable on your repo (if your repo is public). This is especially helpful if you’re working on a shared repository, or your code is being used as a library. You can also add labels and milestones to issues, reference specific commits, or assign issues to specific users. I use issues to track features and bugs even in small projects that I work on alone because they’re a good way to (1) have a to do list and (2) have conversations with myself about what I tried that worked (or not) in relation to a specific issue.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Forks&lt;/p&gt;

    &lt;p&gt;&lt;strong&gt;Scenario&lt;/strong&gt;: You really admire the work that Simon does in his &lt;a href=&quot;https://github.com/SimonGoring/NonAnalogues&quot;&gt;NonAnalogues&lt;/a&gt; and you want to use it in your work. You think that some serious modifications are necessary to the codebase to make it work for your specific application.  Simon is a pretty scary guy, though, so you don’t want to approach him about collaborating on your project, you just want to make the changes yourself.  You have two options: 1) you can download a .zip archive of all the files in their current form from Github and go from there or 2) you can make a &lt;em&gt;fork&lt;/em&gt; of his repository, which creates a new repository under your account, with all of the content from Simon’s project. In essence, a fork is a copy of a repository that allows you to freely experiment with changes without affecting the original project.  Github suggests that forks are most commonly used to either propose changes to someone else’s ideas or to use some else’s project as a starting point for your own idea.  Forks can be superior to downloading the raw files, because they let you stay in sync with the &lt;em&gt;upstream&lt;/em&gt; (original) repository. In this example, if Simon made a bug fix to his NonAnalogues repo at some point in the future, you could sync that change so that you didn’t need to fix that by yourself. Every public repository on Github can be forked by any user.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;collaboration-on-github&quot;&gt;Collaboration on Github&lt;/h4&gt;
&lt;p&gt;Github provides an excellent platform for collaborative work.  There are two models of collaboration that work well on Github. Now that we’ve talked about some of the more advanced features of Github, I think they will make more sense. The content here is adapted from the &lt;a href=&quot;http://cyber4paleo.github.io/resources/collaborating.html&quot;&gt;Cyber4Paleo (C4P)&lt;/a&gt; workshop resources from the Summer of 2016.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Fork and Pull
  This is the model that works best for large, many user projects require a stable branch for production (e.g., a code library). A user interested in collaborating on the project forks the repository, makes changes as she wants, then makes a pull request.  The maintainers of the project test the new code, make comments and changes as they see fit, and then merge the changes into the repository. This model allows for better project management, since and administrator can reject changes, ask for revisions, or decide the order in which changes are incorporated.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Shared Repository
  This model is better suited to small projects with intimate/trustworthy teams. In this case, everyone works on the same repository without a forked repository as an intermediary. A user, who has already been vetted as a collaborator on the project, pulls the most recent version of the repository (using clone) onto her machine, makes changes, and then pushes back to the repository, immediate incorporating changes into the parent repository. This model only works with good communication.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;With slightly larger teams, the shared repository starts to break down.  This can be overcome somewhat by using separate branches, which are then merged together into a master branch.  This allows users to work on their own features and changes, while still contributing to a single repo.&lt;/p&gt;
</description>
        <pubDate>Sat, 10 Sep 2016 09:22:52 -0500</pubDate>
        <link>/tutorial/2016/09/10/Williams-Lab-Github-Lesson.html</link>
        <guid isPermaLink="true">/tutorial/2016/09/10/Williams-Lab-Github-Lesson.html</guid>
        
        
        <category>Tutorial</category>
        
      </item>
    
      <item>
        <title>Ecological Data: Is it &#39;Big Data&#39;?</title>
        <description>&lt;p&gt;We’ve all heard the term ‘Big Data’, though it’s often thrown around as a techy buzzword, along with others, like ‘The Cloud’, without a clear meaning.  In the Williams Lab, we’re working with datasets that are sometimes called ‘Big Data’ in talks by &lt;a href=&quot;https://twitter.com/iceageecologist&quot;&gt;@iceageecolgist&lt;/a&gt; and others, housed in databases like &lt;a href=&quot;http://neotomadb.org&quot;&gt;Neotoma&lt;/a&gt;, &lt;a href=&quot;http://gbif.org&quot;&gt;the Global Biodiversity Information Facility&lt;/a&gt;, and the &lt;a href=&quot;http://paleobiodb.org&quot;&gt;Paleobiology Database&lt;/a&gt;.  Today, I ask, what characteristics of our data make it ‘Big Data’?&lt;/p&gt;

&lt;h3 id=&quot;problem-and-scope&quot;&gt;Problem and Scope&lt;/h3&gt;
&lt;blockquote&gt;
  &lt;p&gt;Question: Can ecological biodiversity data fit under the rubric of Big Data? If so, what are the characteristics that make it Big?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;First, let’s put a limit on the scope of the problem.  Ecology generally has many different subfields, each with their own data and data types.  Some of these may be particularly large, as in the case of ecological modelers, and some may be smaller.  For the sake of argument today, I’ll limit the discussion to ecological biodiversity data documenting &lt;em&gt;occurrences&lt;/em&gt;.   Each occurrence comes with metadata describing what species (typically, but could also be to another taxonomic grouping) was encountered, where it was encountered, and when it was encountered. This type of data is pervasive in the field, and can be used in a host of analyses, including modeling, climate change assessment, and hypotheses testing. Recently, there have been large international campaigns to aggregate these records into large, structured databases that facilitate global biodiversity syntheses.  Three that are commonly encountered are Neotoma (Quaternary), GBIF (modern and instrumental period), and PBDB (deep time).  Since PBDB’s &lt;code class=&quot;highlighter-rouge&quot;&gt;R&lt;/code&gt; package was hard to use, I investigate the question today using data from Neotoma and GBIF.&lt;/p&gt;

&lt;h3 id=&quot;definitions-of-big-data&quot;&gt;Definitions of Big Data&lt;/h3&gt;
&lt;p&gt;There are two often-encountered, decidedly non-technical, designations of Big Data.  The first comes from Wikipedia&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Big data is a term for data sets that are so large or complex that traditional data processing applications are inadequate.&lt;/p&gt;

  &lt;p&gt;(Wikipedia)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This is commonly seen in the marketing materials surrounding big computation and the cloud, though it’s really not a definition at all.  It doesn’t say much about what it is, just that ‘traditional’ means are not capable of processing it, pointing towards distributed computing, cloud computing, and other recent technological advances as its facilitator.  We do get a couple things from this definition though. We know that we’re looking at discrete data sets, partitioned, presumably, in a logical manner.  We’re looking for data sets that ‘traditional’ data processing applications are not feasible.  By using these words, ‘large’ and ‘traditional’, in particular, we can see that ‘Big Data’ is in the eye of the beholder, so to speak, and it depends on your tradition of data processing whether a new dataset is Big or not.  Guterman (2009) suggests, “for some organizations, facing hundreds of gigabytes of data for the first time may trigger a need to reconsider data management options. For others, it may take tens or hundreds of terabytes before data size becomes a significant consideration.”  From Guterman’s perspective, the focus is really on the number of bytes a dataset has, but as we’ll see in a minute, there can be other important factors that comprise a data set’s Bigness.&lt;/p&gt;

&lt;p&gt;The second defintion comes from Yang and Huang’s 2013 book Spatial Cloud Computing:
&amp;gt;Big Data refers to the four V’s: volume, velocity, veracity, and variety.
&amp;gt;
&amp;gt;(Yang and Huang, 2013)&lt;/p&gt;

&lt;p&gt;A varient of this definition can be traced back to an early IBM report on the topic, and can be seen in a variety of cheesy infographics, &lt;a href=&quot;http://www.ibmbigdatahub.com/infographic/four-vs-big-data&quot;&gt;like this one&lt;/a&gt;.  Yang and Huang go on to further describe the meaning of the four V’s, noting that “volume refers to the size of the data; velocity indicates that big data are sensitive to time, variety means big data comprise various types of data with complicated relationships, and veracity indicates the trustworthiness of the data” (p 276).  Here we get a bit more structure than the wikipedia definition gives us, and with the two together, we have a pretty good rubric on which to look at biodiversity datasets.&lt;/p&gt;

&lt;h2 id=&quot;evaluation&quot;&gt;Evaluation&lt;/h2&gt;

&lt;h4 id=&quot;wikipedia&quot;&gt;Wikipedia&lt;/h4&gt;
&lt;p&gt;I argue that the very existence of complex relational databases, like GBIF, Neotoma and PBDB, suggest that biodiversity data do fall under the category of Big Data, as the traditional means of analyzing these data are possible anymore.  Of course, ‘complex’ in the context of the wikipedia statement typically refers to the preponderance of unstructured data, like videos and photos, and ‘large’ usually means too big to fit into a computer’s memory and/or storage drives.  From this perspective, our data is not complex, rather it’s stored in really organized relational tables, and fairly small (the entire Neotoma SQL dump can be downloaded at only 43MB).&lt;/p&gt;

&lt;p&gt;But, if we keep in mind that big data can mean different things to different people, then from our perspective in ecology, our data is Big. Consider the complexity of the relationships between different data records, for example. Figure 1 shows the Neotoma relational table structure, and the complicated web of relationships between each entity.  The data is both spatial and temporal, requiring these attributes, which are known to be messy (see “Veracity”), along with sample data and metadata.  Now, consider keeping track of this for tens of thousands (Neotoma) or hundreds of millions (GBIF) or records, among thousands of independent researchers, and we see why non-traditional techniques like these databases have been developed. Further developments, like APIs and R packages, are even more recent developments to further simplify the tasks of accessing, filtering and working with the datasets. No, ecological biodiversity data does not meet the scale and extent of YouTube, Twitter, or Amazon, but it does require new, custom built tools to store, analyze, and use.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.neotomadb.org/uploads/NeotomaDMD.pdf&quot;&gt;&lt;img src=&quot;/assets/bigData/Neotoma_ER.jpg&quot; alt=&quot;Neotoma_ER&quot; /&gt;&lt;/a&gt;
&lt;em&gt;Figure 1: Neotoma’s Relational Table Structure&lt;/em&gt;&lt;/p&gt;

&lt;h4 id=&quot;volume&quot;&gt;Volume&lt;/h4&gt;
&lt;p&gt;Of the four V’s, the one that most comes to mind when considering what is, or is not, Big Data is volume: how much data is there?  As the quote from Guterman (2009) suggests, some experts consider this to be the only factor in determining what makes data Big. Our datasets are not on the scale of billions of hours of YouTube videos or hundreds of billions of Tweets, but the scale of biodiversity data has exploded in recent years, bringing it to a place where the volume alone is challenging to manage.&lt;/p&gt;

&lt;p&gt;Since the late 1990s, biodiversity databases have quickly and decisively increased the amount of data available to ecologists. Consider Figures 2 and 3, tracking the growth in collections of Neotoma and GBIF through time.  In 1990, only 2 of the records now stored in Neotoma were in digitized collections.  Today, there are over 14,000 datasets.  Each dataset is comprised of spatial and temporal metadata, along with one or more samples with data and associated metadata. The growth rate averages out to about 1.4 datasets every single day for over 26 years.  Considering the time, effort, and money that goes into working up a sediment core (or any of the other data types in Neotoma) this is a really impressive growth rate. For an interesting perspective on ecological Big Data’s reliance on blood, sweat, and tears, take a look at this &lt;a href=&quot;https://contemplativemammoth.com/2013/07/10/is-pollen-analysis-dead-paleoecology-in-the-era-of-big-data/&quot;&gt;Blog Post&lt;/a&gt; by former Williams Labber Jacquelyn Gill.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/bigData/Neotoma_Growth.png&quot; alt=&quot;Neotoma_Growth&quot; /&gt;
&lt;em&gt;Figure 2: Cumulative number of datasets in Neotoma&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The scale of GBIF is on an entirely different level than Neotoma (perhaps because some of the data gathering challenges faced in getting paleo data don’t apply as strongly to modern data collection). Today, GBIF houses digital records of well over 500 million observations, recorded specimens (both fossil and living), and occurrences noted in the scientific literature. GBIF’s records are largely comprised of museum collections, which allow their digital collection to date back to before 1900. The facility itself was introduced in 1999 and officially launched in 2001.  Since 2001, the facility’s holdings have grown nearly 300%, from about 180 million in 2001 to just shy of 614 million occurrence records today.  Managing 613+ million records and associated metadata, and comping with such a fast growth rate, is, without a doubt, a data management challenge worthy of Big Data classification.  Figure 3 shows the exponential growth in GBIF’s holdings since AD 1500, and Figure 4 is an interactive map showing the changes in spatial distribution of their observed data since the late 1800’s.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/bigData/GBIF_Growth.png&quot; alt=&quot;GBIF_Growth&quot; /&gt;
&lt;em&gt;Figure 4: Exponential growth of occurrence records in GBIF&lt;/em&gt;&lt;/p&gt;

&lt;link rel=&quot;stylesheet&quot; href=&quot;https://npmcdn.com/leaflet@0.7.7/dist/leaflet.css&quot; /&gt;

&lt;script src=&quot;https://npmcdn.com/leaflet@0.7.7/dist/leaflet.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://code.jquery.com/jquery-3.1.0.slim.min.js&quot; integrity=&quot;sha256-cRpWjoSOw5KcyIOaZNo4i6fZ9tKPhYYb6i5T9RSVJG8=&quot; crossorigin=&quot;anonymous&quot;&gt;&lt;/script&gt;

&lt;div id=&quot;map&quot; style=&quot;height:500px;&quot;&gt;
&lt;/div&gt;
&lt;p&gt;1890&lt;input type=&quot;range&quot; min=&quot;1890&quot; max=&quot;2016&quot; step=&quot;1&quot; style=&quot;width:50%; display:inline-block; vertical-align:middle&quot; id=&quot;gbif_range&quot; /&gt;2016
&lt;script src=&quot;/assets/gbif_map.js&quot;&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 4: Interactive – Spatial Distribution of GBIF Holdings Through Time&lt;/em&gt;&lt;/p&gt;

&lt;h4 id=&quot;variety&quot;&gt;Variety&lt;/h4&gt;
&lt;p&gt;The second characteristic of Big Data in the four V’s framework is the Variety of the data, and its ‘various types with complicated relationships’ (Yang and Huang). Biodiversity data is highly diverse with many very complicated relationships and interrelationships.&lt;/p&gt;

&lt;p&gt;Neotoma’s holdings range from XRF measurements, to geochronologic data, to fossil vertebrates, to modern pollen surface samples.  In total, there are 23 dataset categories in the database, with more being added from time to time. Though it is structured similarly in the database tables, each of these data types comes from a different community of researchers, using different methods and instruments. Figure 5 shows the breakdown of dataset types in the database.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/bigData/Neotoma_types.png&quot; alt=&quot;Neotoma_Record_types&quot; /&gt;
&lt;em&gt;Figure 5: Dataset Type Breakdown of Neotoma’s Holdings&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;GBIF has 9 defined record type categories, including human observation, living specimen, literature review, and machine measurements.  As with the Neotoma dataset types, these are wildly different from each other.  A living specimen is clearly a totally different type of data to work with than something was derived from a literature review. Yet all of these types coexist together in these large biodiversity datasets. Figure 6 shows how GBIF’s records are distributed amongst these nine types.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/bigData/gbif_types.png&quot; alt=&quot;GBIF&quot; /&gt;
&lt;em&gt;Figure 6: Dataset Type Breakdown of GBIF Holdings&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;To further add to the variety and complexity of our data, it is both spatial and temporal in nature, causing complicated interrelationships between data entities. 87.6 % of GBIF’s records are georeferenced to a real place in the world. 100% of Neotoma’s datasets have spatial information. In these databases, the spatial information is compounded by other fields that describe the location of the observation.  For example, Neotoma has fields describing the site where the fossil was found – it’s altitude, environment, area.  PBDB has extensive metadata for depositional environment, giving additional context to fossil occurrences.  GBIF often notes somewhat colloquial location descriptions in addition to geographic coordinates.   And, of course, there are the relationships between the spatial coordinates themselves – are these things in the same place? do they overlap?&lt;/p&gt;

&lt;p&gt;Managing data with a spatial component is nearly always more challenging than managing data without it. Figure 7 shows how the spatial locations of the datasets contained in Neotoma have changed through time.  Note the expansion in Europe and eastern Asia, and the lack of datasets in Africa.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/assets/bigData/neotoma_spatial_dist.png&quot;&gt;&lt;img src=&quot;/assets/bigData/neotoma_spatial_dist.png&quot; alt=&quot;Neotoma_Maps&quot; /&gt;&lt;/a&gt;
&lt;em&gt;Figure 7: Spatial distribution of additions in Neotoma since 1990&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;A final point on variety is that each record, though now cleanly structured and easily accessed as a record in a database, represents the work of an individual researcher.  The controlled vocabularies and organization policies enforced by the databases have helped to efficiently aggregate the data, however, nearly every record was collected, worked up, and published by a unique individual.  Figure 8 shows the number of datasets attributed to each PI in Neotoma.  Yes the names are too small to read.  The point, though, is that while a couple researchers have a very large number of datasets credited to them (John T Andrews has the most with 335), most have many fewer.  The median number of datasets contributed is 2, and the 3rd quartile value is just 7.  Each researcher will use different equipment, in a different way, call things different names, and generally just do things slightly differently – yielding a highly variable dataset.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/assets/bigData/Neotoma_PIs.png&quot;&gt;&lt;img src=&quot;/assets/bigData/Neotoma_PIs.png&quot; alt=&quot;GBIF&quot; /&gt;&lt;/a&gt;
&lt;em&gt;Figure 8: Neotoma dataset submissions by principle investigator&lt;/em&gt;&lt;/p&gt;

&lt;h4 id=&quot;veracity&quot;&gt;Veracity&lt;/h4&gt;
&lt;p&gt;Ecological data has high levels of uncertainty associated with it.  Some can be estimated, like temporal and spatial uncertainty.  Others are less amenable to being quantified, for example inter-researcher identification differences, measurement errors, and data lost in the transition from field to lab to database. See &lt;a href=&quot;http://www.sciencedirect.com/science/article/pii/S0277379116300142#appsec1&quot;&gt;this paper&lt;/a&gt; for a Paleon project that used expert elicitation to quantify the differences between the dates assigned to European settlement horizon, a process they argue varies between sites, and depends on the “temporal density of pollen samples, time-averaging of sediments, the rapidity of forest clearance and landscape transformation, the pollen representation of dominant trees, which can dampen or amplify the ragweed signal, and expert knowledge of the region and the late-Holocene history of the site.” The raw data from the expert elicitation is included as supplementary information in their paper, and can be seen to vary pretty significantly between the four experts.&lt;/p&gt;

&lt;p&gt;Some information will be lost in the process of going from a field site through a lab workflow to being aggregated in the dataset.  Not all process details can be incorporated into database metadata fields, and probably more importantly, contextual details essential to proper interpretation of the data often gets lost on aggregation.&lt;/p&gt;

&lt;p&gt;Coincidentally, when I start working on my PhD here at UW, I’ll be working to tackle some of these uncertainty issues.&lt;/p&gt;

&lt;p&gt;To illustrate the veracity (or lack thereof) of the biodiversity data, let’s look at spatial coordinate uncertainty in GBIF and temporal uncertainty of chronological control points in Neotoma. The GBIF database, in addition to recording the geographic coordiantes of an occurrence, also includes a field for uncertainty in spatial location, though this field is optional.  I downloaded 10,000 records of the genus &lt;em&gt;Picea&lt;/em&gt;, of which over half did not include this field (though all were georeferenced).  This means that even if I am able include and propagate uncertainty in my models (as in Bayesian Hierarchical Models), I would be unable to do so really effectively, because few researchers even report this field. Of the 4,519 records that did report &lt;code class=&quot;highlighter-rouge&quot;&gt;coordinateUncertaintyInMeters&lt;/code&gt;, the average uncertainty was 305m (if you exclude zero, which seems reasonable to do). The maximum uncertainty in this dataset was 1,970m.  From this brief, and admittedly flawed, assessment, we can see there are some pretty serious problems with using the coordinates without considering their uncertainty first.  If, for example, you’re using 800m gridded climate model output to look at environmental covariates to species presence (which I do), a 300m uncertainty in species location could cause significant deviations due to gridcell mis-assignment, particularly in mountainous regions like the Western U.S.&lt;/p&gt;

&lt;p&gt;On the temporal side of things, we can do a similar assessment, this time using the Neotoma data.  Neotoma samples are assigned an age using age controls (like radiocarbon dates or varve counts) or an age model, which interpolates between the age controls. The age model issue is a challenging one, and there’s a lot of literature out there about it, as well as software to improve from simple linear models. Every age model is based on a set of age controls, which often have uncertainty associated with them.   Neotoma records an minimum and maximum age for each age control for each dataset.  Out of a sample of 32,341 age controls in the database, only 5,722 reported age uncertainty.  Some record types, like varves, can perhaps be assigned an uncertainty of zero, so we can safely ignore 2,830 more controls, leaving us with 2,892 that report values for minimum and maximum age. The summary statistics for these age controls suggest that the median age model tie point has a temporal uncertainty of 260.0 years. The 25% percentile is an uncertainty of 137.5 years and the 75% 751.2 years.  Using the mean of 260.0 years, I suggest that we can only identify down to ± 130 years of the actual date.  Considering sediment mixing, laboratory precision, and other processes at work, maybe this isn’t that big of a deal, but it definitely is something to be aware of and contributes to biodiversity data’s lack of absolute veracity.&lt;/p&gt;

&lt;h4 id=&quot;velocity&quot;&gt;Velocity&lt;/h4&gt;
&lt;p&gt;The final piece of the framework is the data’s velocity – how time sensitive is the data.  Data’s velocity important because high velocity data must be analyzed as a stream.  Tweets, for example, must be analyzed for trends as they are posted. Knowing the trending topics of two weeks ago might be interesting to me, but the real draw of a Big Data platform like twitter is that I can participate in the trending topics of &lt;em&gt;right now&lt;/em&gt;.  To do such an analysis, one must use sophisticated sampling techniques and algorithms to detect clusters and trends in real time, for example &lt;a href=&quot;http://jmlr.csail.mit.edu/proceedings/papers/v17/bifet11a/bifet11a.pdf&quot;&gt;this paper&lt;/a&gt;, which comments on sampling strategies used for trend detection.&lt;/p&gt;

&lt;p&gt;This is the one area where I would suggest that ecological biodiversity data is not Big Data.  Biodiversity analyses, like species distribution models, at least the ones I am familiar with, usually take between a few minutes and a few days to complete and are not especially time sensitive.  The rate of increase in data volume in both Neotoma and GBIF is not fast enough to invalidate the results from previous analyses.  Neotoma gets approximately 1.4 new datasets each day (1990-2016 average).  GBIF gets about 59,000 new occurrences each day (2000-2015 average).  Sure, that’s a lot of new datasets, but the likelihood you would actually use the new data in a given analysis is low, and the likelihood that its immediately inclusion into a new model would significantly change your conclusions is even lower.&lt;/p&gt;

&lt;p&gt;The velocity of data coming into the databases, particularly into GBIF, is staggering, no doubt about it.  Nonetheless, I don’t think it it warrants the use of specialized streaming algorithms for extracting information from the new data points.  I have not seen anyone attempt to do such a thing (though maybe this would be an interesting experiment?).  Moreover, there is little incentive to immediately analyze the data, because there is next to nothing to be gained from modeling biodiversity faster than you can report your results in publications.&lt;/p&gt;

&lt;h3 id=&quot;so-is-it&quot;&gt;So, is it?&lt;/h3&gt;
&lt;p&gt;Velocity notwithstanding, biodiversity occurrence data passes four of five facets of the Big Data, so I conclude that, &lt;strong&gt;yes, it is big data.&lt;/strong&gt; It requires specialized databases and software to interact with it, it has large numbers of records, it is extremely diverse, and it has high levels of uncertainty with which to deal.&lt;/p&gt;

&lt;p&gt;Looking forward, I suspect Big Data will continue to challenge those involved in synthetic research. Perhaps one of the most challenging aspects is the relatively short period of time in which these data became Big. Figures 9 and 10 show the annual increase in holdings for Neotoma (Fig. 9) and GBIF (Fig 10) through time (top) and the rate of change of annual increase (bottom). While Neotoma’s rate of increase as remained relatively steady through time (clear from the near-linear trend in Figure 2), GBIF’s rate shows a significant upward trend in the last several years.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/assets/bigData/Neotoma_growth_diff.png&quot;&gt;&lt;img src=&quot;/assets/bigData/Neotoma_growth_diff.png&quot; alt=&quot;Neotoma_Delta&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 9: Neotoma holdings, Annual Change and Rate of Change of Annual Change&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/assets/bigData/gif_growth_diff.png&quot;&gt;&lt;img src=&quot;/assets/bigData/gif_growth_diff.png&quot; alt=&quot;GBIF_Delta&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 10: GBIF holdings, Annual Change and Rate of Change of Annual Change&lt;/em&gt;&lt;/p&gt;
</description>
        <pubDate>Wed, 31 Aug 2016 08:22:52 -0500</pubDate>
        <link>/research/paleo/2016/08/31/Big-Data-In-Ecology.html</link>
        <guid isPermaLink="true">/research/paleo/2016/08/31/Big-Data-In-Ecology.html</guid>
        
        
        <category>Research</category>
        
        <category>Paleo</category>
        
      </item>
    
      <item>
        <title>Adding Shareable URLs to IceAgeMapper</title>
        <description>&lt;p&gt;One of the features Rob suggested I add to Ice Age Mapper during our last meeting was a dynamic url that would record the current state of the application, and could thus be shared between users. I took a stab at that last week, and got it working pretty well.  I thought it would be a lot of re-coding from the ground up, but it turns out that most of what I had written previously could be easily converted to load a URL string.  My application only generates a shareable URL when the user clicks the ‘Share’ button, but in theory, the app could easily be modified to generate a new URL each time an action was taken.  I think this would actually &lt;strong&gt;&lt;em&gt;Not&lt;/em&gt;&lt;/strong&gt; be a good idea, because it would mean there would be an entry in the user’s web history for each action they took inside of the application, meaning they would have to click the back button like a million times if they messed up.  Good to know support exists for that though.&lt;/p&gt;

&lt;p&gt;Another plus of designing a dynamic state URL is that it can be used to share the current configuration on Twitter, GooglePlus, by email, or other social media.  While not critical for our purposes, it seems like it’s never a bad thing to tap into social channels.&lt;/p&gt;

&lt;p&gt;There are two main parts of implementing a dynamic state url: generating and parsing.  The generating phase includes functions that get the current state of the system and translate them into a URL variable, and then string the URL variables together into a complete URL.  In the parsing phase, the URL variable parse (or not, if they don’t exist) and translate them into function calls to re-generate the desired state.  Before starting to code, make a list of the parts of the state you want to keep track of.  Do you want to keep track of every click made to get to a certain configuration or just the configuration itself?&lt;/p&gt;

&lt;p&gt;I decided I want to keep track of the following parts of the application state:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;em&gt;Taxon&lt;/em&gt;: the data that is currently being displayed, as returned from a Neotoma API call.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Map Center&lt;/em&gt;: Geographic center of current map view, as latitude and longitude.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Map Zoom&lt;/em&gt;: Zoom level of current map view.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Minimum Year&lt;/em&gt;: Minimum (most recent) year in temporal view.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Maximum Year&lt;/em&gt;: Maximum (most distant) year in temporal view.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Panel Configuration&lt;/em&gt;: For each panel, is it open or closed?  Currently, there are three panels: Taxonomy, Site, and NicheViewer.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Layer Configuration&lt;/em&gt;:  For each layer, is it visible or hidden?  Currently, there are three layers: Ice Sheets, Sites, and Heatmap.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;You’ll likely find that there are things you want to add to the state at a later time, but with the general framework, such additions should be really easy.&lt;/p&gt;

&lt;h3 id=&quot;part-1-generation&quot;&gt;Part 1: Generation&lt;/h3&gt;
&lt;p&gt;To generate the URL, I used the &lt;code class=&quot;highlighter-rouge&quot;&gt;URI.js&lt;/code&gt; library.  This &lt;a href=&quot;https://medialize.github.io/URI.js/&quot;&gt;library&lt;/a&gt; makes it easy to parse, add to, and validate URL strings on the current window, or another window.  Generate a new URI for the current window location:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-js&quot; data-lang=&quot;js&quot;&gt;  &lt;span class=&quot;nx&quot;&gt;uri&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;URI&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;And then add query variables as needed, using:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-js&quot; data-lang=&quot;js&quot;&gt;  &lt;span class=&quot;nx&quot;&gt;uri&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;addQuery&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&#39;key&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;value&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The next, and most intensive step in this phase to get the values of each state component at this point in time.  For some, like the leaflet map center and zoom, it will be easy to do this, because the leaflet map object already tracks these for you (&lt;code class=&quot;highlighter-rouge&quot;&gt;map.getCenter()&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;map.getZoom()&lt;/code&gt;).  Depending on your coding style, you may already have pointers to some of the components, or you may need to devise a way of going to get the values.  Because I chose to only generate the new state URL once the user has requested it, we can write some functions to go get the values at the time they click the button. Mostly, though, I use a &lt;code class=&quot;highlighter-rouge&quot;&gt;globals&lt;/code&gt; object, that keeps references to a variety of important properties that I might want to access throughout my code.  I think this is a good compromise between having a ton of global variables floating around, and totally scoping the variables into functions.  Maybe I’m wrong, not sure…&lt;/p&gt;

&lt;p&gt;Anyways, for each of my state components, I go get it’s value, and then set it to our new &lt;code class=&quot;highlighter-rouge&quot;&gt;uri&lt;/code&gt; object.  Remember that the properties should be Boolean, String, or Numeric types, rather than object or something else that can’t be easily serialized into the URI.  This can be a little tricky, but it’s important so think about how you can make it work.  For example, if I want to populate a panel with data, I can’t easily serialize the data into the URI string.  Instead, I tell the URL that I do want to populate that panel, and I want that panel to automatically open.  Then I write re-write the panel function so that it can automatically call Neotoma and populate the details with the API call results. More on that in the next section.&lt;/p&gt;

&lt;p&gt;When the URI component contains all of your desired state components, you can get it’s value by calling &lt;code class=&quot;highlighter-rouge&quot;&gt;uri.toString()&lt;/code&gt;.  If you set &lt;code class=&quot;highlighter-rouge&quot;&gt;window.location.href=uri.toString()&lt;/code&gt; you will reload the page.  If that’s what you want, go for that.  In my case, I set a text box to the value of the &lt;code class=&quot;highlighter-rouge&quot;&gt;toString()&lt;/code&gt; method, which users can copy and paste if they want. In addition, I do add a history entry into the user’s browser history.  This is accomplished by:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-js&quot; data-lang=&quot;js&quot;&gt;  &lt;span class=&quot;nb&quot;&gt;window&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;history&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;pushState&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Ice Age Mapper&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;Ice Age Mapper&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;uri&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;toString&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;That’s about all there is on the generation side of things.  The more involved coding comes when trying to parse a share url.&lt;/p&gt;

&lt;h3 id=&quot;part-2-parsing&quot;&gt;Part 2: Parsing&lt;/h3&gt;
&lt;p&gt;Once you have a URL, you need to put in the infrastructure to generate the state that the URL calls for.  First though, you need to read the url string and parse it into its component parts.  To read the URL, I found that this function was super helpful (I borrowed it from &lt;a href=&quot;http://stackoverflow.com/questions/901115/how-can-i-get-query-string-values-in-javascript&quot;&gt;this StackOverflow post&lt;/a&gt;):&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-javascript&quot; data-lang=&quot;javascript&quot;&gt;  &lt;span class=&quot;kd&quot;&gt;function&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;getURLParameterByName&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;url&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;!&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;url&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;url&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;window&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;location&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;href&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
      &lt;span class=&quot;nx&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;replace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sr&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;[\[\]]&lt;/span&gt;&lt;span class=&quot;sr&quot;&gt;/g&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;\\$&amp;amp;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
      &lt;span class=&quot;kd&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;regex&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;RegExp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;[?&amp;amp;]&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;(=([^&amp;amp;#]*)|&amp;amp;|#|$)&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
          &lt;span class=&quot;nx&quot;&gt;results&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;regex&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;exec&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;url&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;!&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;results&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;null&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;!&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;results&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&#39;&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;decodeURIComponent&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;results&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;replace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sr&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\+&lt;/span&gt;&lt;span class=&quot;sr&quot;&gt;/g&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot; &quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;hr /&gt;
&lt;p&gt;You can then get the query parameters from the URL string like so:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-javascript&quot; data-lang=&quot;javascript&quot;&gt;  &lt;span class=&quot;kd&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;queryVar&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;getURLParameterByName&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;key&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;For each state component, I parsed the URI variable associated with it.  I also added some checks to make sure that if the query was not in the URI, the application wouldn’t crash, but would instead default to something smart.  For example, to get the currently displayed taxon:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-javascript&quot; data-lang=&quot;javascript&quot;&gt;  &lt;span class=&quot;nx&quot;&gt;taxon&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;getURLParameterByName&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&#39;taxon&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;taxon&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
    &lt;span class=&quot;nx&quot;&gt;globals&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;taxon&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;taxon&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;//set the name in the search box&lt;/span&gt;
    &lt;span class=&quot;nx&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;#searchBar&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;taxon&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;toProperCase&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
    &lt;span class=&quot;nx&quot;&gt;globals&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;autoload&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Once all of the query variables have been parsed, we need a way of translating the new state information into the actual application state.  I do this in two steps.  First, I have a function that does all of the parsing.  During the parsing, the global variable objects gets property values for the configuration (e.g., &lt;code class=&quot;highlighter-rouge&quot;&gt;globals.taxon = &#39;Quercus&#39;&lt;/code&gt;).  Next, I call a load function, which is pretty much the same as what I had when I didn’t allow URL configuration, but instead of just setting the variables to &lt;code class=&quot;highlighter-rouge&quot;&gt;Null&lt;/code&gt; at the start, it checks to see if the property has already been set during the parsing phase.  This method works really well for components like map zoom and time extent.  However, it will not automatically load the data from Neotoma, because loading the data requires a button click to send an AJAX request to the Neotoma API.  Therefore, we add a &lt;code class=&quot;highlighter-rouge&quot;&gt;globals.autoload&lt;/code&gt; property, which automatically triggers a click on that button, if the necessary state configurtion variables (like taxon) are set in the URL.&lt;/p&gt;

&lt;h3 id=&quot;part-3--sharing-on-social-media&quot;&gt;Part 3:  Sharing on Social Media&lt;/h3&gt;
&lt;p&gt;One you’ve implemented the generating and parsing, and know that your share URL gives you a reliable application state representation, you can share the URL on twitter or other social media really easily.&lt;br /&gt;
#### Copying to the clipboard
While not social, you may wish to allow users to copy the link directly to their copy-paste clipboard.  I read some discussion of how this may be a bad idea for security.  I’m not sure – I added it anyways.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-js&quot; data-lang=&quot;js&quot;&gt;&lt;span class=&quot;c1&quot;&gt;// create hidden text element, if it doesn&#39;t already exist&lt;/span&gt;
&lt;span class=&quot;nx&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;#share-link&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;focus&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;//highlight the text element that contains the link&lt;/span&gt;
&lt;span class=&quot;kd&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;succeed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;try&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;nx&quot;&gt;succeed&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;document&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;execCommand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;copy&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;//do the copying&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;catch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
  &lt;span class=&quot;nx&quot;&gt;succeed&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;false&lt;/span&gt;
  &lt;span class=&quot;nx&quot;&gt;console&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;succeed&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;//Boolean&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h4 id=&quot;sharing-on-twitter&quot;&gt;Sharing on Twitter&lt;/h4&gt;
&lt;p&gt;Twitter allows you to configure a link that pre-populates a tweet composer with message body, share url, hashtags, mentions, etc.  This was a little hard to get the hang of, and I still don’t think it’s quite right.  I used the &lt;code class=&quot;highlighter-rouge&quot;&gt;URI.js&lt;/code&gt; library again to generate this URL, and then set it to the &lt;code class=&quot;highlighter-rouge&quot;&gt;href&lt;/code&gt; property of a link.  The &lt;code class=&quot;highlighter-rouge&quot;&gt;generateTwitterLink&lt;/code&gt; function is called right after the share URL is generated, so that it is available to the user if they choose to share on twitter. FYI: Even if you have a really long share url, it will only take up 22 characters of your tweet &lt;em&gt;if you are on a real server&lt;/em&gt;.  If you are on localhost, which isn’t a qualified domain, it will take up all of the characters, so might not work.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-js&quot; data-lang=&quot;js&quot;&gt;&lt;span class=&quot;kd&quot;&gt;function&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;generateTwitterLink&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(){&lt;/span&gt;
  &lt;span class=&quot;kd&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;twitterURL&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;URI&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;http://twitter.com/share/&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;//base link&lt;/span&gt;
  &lt;span class=&quot;nx&quot;&gt;twitterURL&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;addQuery&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;url&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;globals&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;shareURI&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;//prepopulate with a URL&lt;/span&gt;
  &lt;span class=&quot;nx&quot;&gt;twitterURL&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;addQuery&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;text&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;Check out my Ice Age Map!&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;//prepopulate text&lt;/span&gt;
  &lt;span class=&quot;nx&quot;&gt;twitterURL&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;addQuery&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;hashtags&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;paleo&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;nx&quot;&gt;twitterURL&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;twitterURL&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;toString&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;//generate the string from the object&lt;/span&gt;
  &lt;span class=&quot;nx&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;.twitter-share-button&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;attr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;href&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;twitterURL&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;//set the link attribute so we actually use the dynamic URL&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h4 id=&quot;sharing-with-email&quot;&gt;Sharing with Email&lt;/h4&gt;
&lt;p&gt;Perhaps the most likely way to share an application state for this application is by email, so I added a method that you can easily email the link out to your collaborators from inside the app. This is super easy, you just need to set the subject and body of the &lt;code class=&quot;highlighter-rouge&quot;&gt;mailto:&lt;/code&gt; string inside of the link &lt;code class=&quot;highlighter-rouge&quot;&gt;href&lt;/code&gt;. Since we don’t know who to send it to, we leave the &lt;code class=&quot;highlighter-rouge&quot;&gt;to:&lt;/code&gt; field blank.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-js&quot; data-lang=&quot;js&quot;&gt;&lt;span class=&quot;nx&quot;&gt;link&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;mailto:?to=&amp;amp;&quot;&lt;/span&gt;
&lt;span class=&quot;nx&quot;&gt;link&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;subject=&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;encodeURIComponent&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Ice Age Mapper&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;//changes spaces to %20, etc&lt;/span&gt;
&lt;span class=&quot;nx&quot;&gt;link&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&amp;amp;body=&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;shareURI&lt;/span&gt;
&lt;span class=&quot;nx&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;#emailLink&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&#39;href&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;link&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h4 id=&quot;sharing-on-google&quot;&gt;Sharing on Google+&lt;/h4&gt;
&lt;p&gt;Sharing on Google+, which I don’t know if anyone actually uses – I don’t–  was really, really easy. Assuming you have the required script included, you can have your sharing element be something like:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-html&quot; data-lang=&quot;html&quot;&gt;div class=&quot;g-plus&quot; data-action=&quot;share&quot;&amp;gt;&lt;span class=&quot;nt&quot;&gt;&amp;lt;/div&amp;gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Then you can enable the sharing with your URL by setting the url data attribute inside of your javascript code, again, immediately after you generate the share URL.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-js&quot; data-lang=&quot;js&quot;&gt;&lt;span class=&quot;nx&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;.g-plus&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&#39;href&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;shareURI&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

</description>
        <pubDate>Sun, 21 Aug 2016 08:22:52 -0500</pubDate>
        <link>/research/paleo/2016/08/21/Adding-Shareable-URLs-To-IceAgeMapper.html</link>
        <guid isPermaLink="true">/research/paleo/2016/08/21/Adding-Shareable-URLs-To-IceAgeMapper.html</guid>
        
        
        <category>Research</category>
        
        <category>Paleo</category>
        
      </item>
    
      <item>
        <title>Updating R on Debian Linux</title>
        <description>&lt;p&gt;Why the &lt;code class=&quot;highlighter-rouge&quot;&gt;apt-get&lt;/code&gt; package manager doesn’t contain the latest version of &lt;code class=&quot;highlighter-rouge&quot;&gt;R&lt;/code&gt; automatically, I’m not sure. I recently realized I have been downloading a 2+ year old distribution for all of my SDM timing runs by running the standard &lt;code class=&quot;highlighter-rouge&quot;&gt;sudo apt-get install r-base&lt;/code&gt; command at the shell.  For several weeks, this was fine, but today the package &lt;code class=&quot;highlighter-rouge&quot;&gt;Rcpp&lt;/code&gt;, which wraps compiled C++ code in the R environment failed to compile.  I spent most of the afternoon trying to figure out what was going on.  I didn’t even occur to me that the  &lt;code class=&quot;highlighter-rouge&quot;&gt;r-base&lt;/code&gt; package I was using was the root cause.&lt;/p&gt;

&lt;p&gt;It is not easy to figure out how to update the core R package, but, like most things in linux, it comes down to a correctly ordered set of calls to a package manager.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Note:&lt;/em&gt; I am using a Debian 8 Jessie image, version v20160718&lt;/p&gt;

&lt;p&gt;###Steps:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo apt-get remove r-base&lt;/code&gt;.  Remove the old version of R.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo nano /etc/apt/sources.list&lt;/code&gt;.  This file holds all of the package repositories for &lt;code class=&quot;highlighter-rouge&quot;&gt;apt-get&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;Inside of it, copy and paste:
    &lt;pre&gt;
deb http://cran.rstudio.com/bin/linux/debian jessie-cran3/
&lt;/pre&gt;

    &lt;p&gt;This tells the manager to look in this repository for a copy of the R distribution.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Save and close the text editor.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;At the shell, type:
  &lt;code&gt;
  gpg --keyserver keyserver.ubuntu.com --recv-key E084DAB9
  &lt;/code&gt;
  and then
  &lt;code&gt;
  gpg -a --export E084DAB9 | sudo apt-key add -
  &lt;/code&gt;
  What does this do? I’m not exactly sure, but I think it has to do with the package integrity checks down when downloading things from a package manager.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo apt-get update&lt;/code&gt;.  Update the installed packages.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo apt-get install r-base&lt;/code&gt;.  Install the core R functionality, hopefully this time using the newest version.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo apt-get install r-base-dev&lt;/code&gt;.  Install the development headers to allow packages that are not in debian repositories.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;At this point, you should have a newly updated R version.  You can check with R.version.  For me, this worked for updating from R version &lt;code class=&quot;highlighter-rouge&quot;&gt;3.0.1&lt;/code&gt; to R version &lt;code class=&quot;highlighter-rouge&quot;&gt;3.3.1&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;If you have package install error, it’s definitely worth checking if an update in the r-base package could be responsible.&lt;/p&gt;
</description>
        <pubDate>Tue, 19 Jul 2016 09:22:52 -0500</pubDate>
        <link>/research/cloudcomputing/2016/07/19/Updating-R-on-Debian.html</link>
        <guid isPermaLink="true">/research/cloudcomputing/2016/07/19/Updating-R-on-Debian.html</guid>
        
        
        <category>Research</category>
        
        <category>CloudComputing</category>
        
      </item>
    
  </channel>
</rss>
